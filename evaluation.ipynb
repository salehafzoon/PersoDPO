{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Installation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install gensim requests rouge bert_score openpyxl prettytable nltk gdown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import nltk\n",
    "# nltk.download('stopwords')\n",
    "# nltk.download('punkt')\n",
    "# nltk.download('punkt_tab')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import gdown\n",
    "# import gzip\n",
    "# import shutil\n",
    "# import os\n",
    "\n",
    "# compressed_path = \"./GoogleNews-vectors-negative300.bin.gz\"\n",
    "# decompressed_path = \"./GoogleNews-vectors-negative300.bin\"\n",
    "\n",
    "\n",
    "# # Google Drive file ID extracted from the link\n",
    "# file_id = \"0B7XkCwpI5KDYNlNUTTlSS21pQmM\"\n",
    "\n",
    "# # Download the file\n",
    "# gdown.download(f\"https://drive.google.com/uc?id={file_id}&export=download\", compressed_path, quiet=False)\n",
    "\n",
    "# print(\"Download completed.\")\n",
    "\n",
    "\n",
    "# # Extract the .bin file from .gz\n",
    "# with gzip.open(compressed_path, \"rb\") as f_in:\n",
    "#     with open(decompressed_path, \"wb\") as f_out:\n",
    "#         shutil.copyfileobj(f_in, f_out)\n",
    "\n",
    "# print(\"Extraction completed.\")\n",
    "\n",
    "# # (Optional) Delete the .gz file after extraction\n",
    "# os.remove(compressed_path)\n",
    "# print(\"Deleted the compressed file.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Performance Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import warnings\n",
    "import logging\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "logging.getLogger('transformers').setLevel(logging.ERROR)\n",
    "\n",
    "# Set the logging level to ERROR to ignore warnings\n",
    "logging.getLogger(\"transformers\").setLevel(logging.ERROR)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET = \"FoCus\"                      # \"Blended Skill Talk\", \"IT-ConvAI2\" , \"FoCus\" \n",
    "\n",
    "_COT = \"\"                                   #  \"\",    \"-COT\"\n",
    "\n",
    "SCORING_METHOD = \"avg\"                      # \"avg\",  \"length_prior\"\n",
    "\n",
    "LLM = f\"Qwen2-5B-{DATASET}-{SCORING_METHOD}\"\n",
    "\n",
    "COT_SETUP = True if _COT == \"-COT\" else False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape: (1000, 3)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>personas</th>\n",
       "      <th>context</th>\n",
       "      <th>act_response</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>I would like to visit the Nazareth House again...</td>\n",
       "      <td>User1: I think Ive been there before but I don...</td>\n",
       "      <td>User2: The history of the house you are intere...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>I have been to Vermont a few times to go skiin...</td>\n",
       "      <td>User1: Wow, this is amazing! What is this?\\nUs...</td>\n",
       "      <td>User2: This house was use as a stop for slaves...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            personas  \\\n",
       "0  I would like to visit the Nazareth House again...   \n",
       "1  I have been to Vermont a few times to go skiin...   \n",
       "\n",
       "                                             context  \\\n",
       "0  User1: I think Ive been there before but I don...   \n",
       "1  User1: Wow, this is amazing! What is this?\\nUs...   \n",
       "\n",
       "                                        act_response  \n",
       "0  User2: The history of the house you are intere...  \n",
       "1  User2: This house was use as a stop for slaves...  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(f'./Prompts/{DATASET}.csv')\n",
    "print(\"Shape:\", df.shape)\n",
    "\n",
    "df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "personas        0\n",
      "context         0\n",
      "act_response    0\n",
      "dtype: int64\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>personas</th>\n",
       "      <th>context</th>\n",
       "      <th>act_response</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>I would like to visit the Nazareth House again...</td>\n",
       "      <td>User1: I think Ive been there before but I don...</td>\n",
       "      <td>The history of the house you are interested in...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>I have been to Vermont a few times to go skiin...</td>\n",
       "      <td>User1: Wow, this is amazing! What is this?\\nUs...</td>\n",
       "      <td>This house was use as a stop for slaves trying...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            personas  \\\n",
       "0  I would like to visit the Nazareth House again...   \n",
       "1  I have been to Vermont a few times to go skiin...   \n",
       "\n",
       "                                             context  \\\n",
       "0  User1: I think Ive been there before but I don...   \n",
       "1  User1: Wow, this is amazing! What is this?\\nUs...   \n",
       "\n",
       "                                        act_response  \n",
       "0  The history of the house you are interested in...  \n",
       "1  This house was use as a stop for slaves trying...  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Only For: FoCus, IT-ConvAI2\n",
    "\n",
    "df['act_response'] = df['act_response'].apply(lambda x: x.split(':', 1)[1].strip() if ':' in x else x.strip())\n",
    "\n",
    "print(df.isnull().sum())\n",
    "df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape: (1000, 2)\n",
      "\n",
      "Missing Values:\n",
      "gen_response     383\n",
      "response_time      0\n",
      "dtype: int64\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>gen_response</th>\n",
       "      <th>response_time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>I'm sorry, but I don't have any personal exper...</td>\n",
       "      <td>0.902046</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>I'm sorry, but I don't see any specific inform...</td>\n",
       "      <td>0.941635</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Great choice for your next visit! The Marion P...</td>\n",
       "      <td>0.887553</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.894501</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>I'm sorry, but I don't understand what you're ...</td>\n",
       "      <td>0.884114</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>995</th>\n",
       "      <td>I'm sorry, but as a participant in this conver...</td>\n",
       "      <td>0.889212</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>996</th>\n",
       "      <td>I'm sorry, but I don't understand what you're ...</td>\n",
       "      <td>0.884870</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>997</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.889985</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>998</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.882518</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>999</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.888940</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1000 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          gen_response  response_time\n",
       "0    I'm sorry, but I don't have any personal exper...       0.902046\n",
       "1    I'm sorry, but I don't see any specific inform...       0.941635\n",
       "2    Great choice for your next visit! The Marion P...       0.887553\n",
       "3                                                  NaN       0.894501\n",
       "4    I'm sorry, but I don't understand what you're ...       0.884114\n",
       "..                                                 ...            ...\n",
       "995  I'm sorry, but as a participant in this conver...       0.889212\n",
       "996  I'm sorry, but I don't understand what you're ...       0.884870\n",
       "997                                                NaN       0.889985\n",
       "998                                                NaN       0.882518\n",
       "999                                                NaN       0.888940\n",
       "\n",
       "[1000 rows x 2 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "COT_ = \"-COT\" if COT_SETUP else \"\"\n",
    "\n",
    "response = pd.read_csv(f'Responses/{DATASET}/{LLM}{COT_}.csv')\n",
    "print(\"Shape:\", response.shape)\n",
    "\n",
    "print(\"\\nMissing Values:\")\n",
    "print(response.isnull().sum())\n",
    "\n",
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Maximum Response Length (in words): 89\n"
     ]
    }
   ],
   "source": [
    "# Calculate maximum number of words in each column\n",
    "max_response_length = response['gen_response'].dropna().apply(lambda x: len(x.split())).max()\n",
    "\n",
    "print(f\"Maximum Response Length (in words): {max_response_length}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "personas           0\n",
      "context            0\n",
      "act_response       0\n",
      "gen_response     383\n",
      "response_time      0\n",
      "dtype: int64\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>personas</th>\n",
       "      <th>context</th>\n",
       "      <th>act_response</th>\n",
       "      <th>gen_response</th>\n",
       "      <th>response_time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>I would like to visit the Nazareth House again...</td>\n",
       "      <td>User1: I think Ive been there before but I don...</td>\n",
       "      <td>The history of the house you are interested in...</td>\n",
       "      <td>I'm sorry, but I don't have any personal exper...</td>\n",
       "      <td>0.902046</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>I have been to Vermont a few times to go skiin...</td>\n",
       "      <td>User1: Wow, this is amazing! What is this?\\nUs...</td>\n",
       "      <td>This house was use as a stop for slaves trying...</td>\n",
       "      <td>I'm sorry, but I don't see any specific inform...</td>\n",
       "      <td>0.941635</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I am fascinated by the Spanish Colonial Reviva...</td>\n",
       "      <td>User1: Wow, this is amazing! What is this?\\nUs...</td>\n",
       "      <td>Sure, you will like to know that this place wa...</td>\n",
       "      <td>Great choice for your next visit! The Marion P...</td>\n",
       "      <td>0.887553</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>I want to become a college student.I want to s...</td>\n",
       "      <td>User1: Where is this place?\\nUser2: Hello! Wel...</td>\n",
       "      <td>Technische Universität Darmstadt in the top 25...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.894501</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>I like to visit england.I love church.I would ...</td>\n",
       "      <td>User1: Where is this place?\\nUser2: This place...</td>\n",
       "      <td>I suggest a place, for your wish of see librar...</td>\n",
       "      <td>I'm sorry, but I don't understand what you're ...</td>\n",
       "      <td>0.884114</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            personas  \\\n",
       "0  I would like to visit the Nazareth House again...   \n",
       "1  I have been to Vermont a few times to go skiin...   \n",
       "2  I am fascinated by the Spanish Colonial Reviva...   \n",
       "3  I want to become a college student.I want to s...   \n",
       "4  I like to visit england.I love church.I would ...   \n",
       "\n",
       "                                             context  \\\n",
       "0  User1: I think Ive been there before but I don...   \n",
       "1  User1: Wow, this is amazing! What is this?\\nUs...   \n",
       "2  User1: Wow, this is amazing! What is this?\\nUs...   \n",
       "3  User1: Where is this place?\\nUser2: Hello! Wel...   \n",
       "4  User1: Where is this place?\\nUser2: This place...   \n",
       "\n",
       "                                        act_response  \\\n",
       "0  The history of the house you are interested in...   \n",
       "1  This house was use as a stop for slaves trying...   \n",
       "2  Sure, you will like to know that this place wa...   \n",
       "3  Technische Universität Darmstadt in the top 25...   \n",
       "4  I suggest a place, for your wish of see librar...   \n",
       "\n",
       "                                        gen_response  response_time  \n",
       "0  I'm sorry, but I don't have any personal exper...       0.902046  \n",
       "1  I'm sorry, but I don't see any specific inform...       0.941635  \n",
       "2  Great choice for your next visit! The Marion P...       0.887553  \n",
       "3                                                NaN       0.894501  \n",
       "4  I'm sorry, but I don't understand what you're ...       0.884114  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import string\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# Initialize stop words\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "# Function to preprocess text\n",
    "def preprocess_text(text, remove_stop_words=True):\n",
    "    if pd.isnull(text):\n",
    "        return None\n",
    "    text = text.lower()  # Lowercasing\n",
    "    text = text.translate(str.maketrans('', '', string.punctuation))  # Removing punctuation\n",
    "    tokens = word_tokenize(text)  # Tokenization\n",
    "    if remove_stop_words:\n",
    "        tokens = [word for word in tokens if word not in stop_words]  # Removing stop words\n",
    "    return ' '.join(tokens)  # Join tokens back into a single string\n",
    "\n",
    "\n",
    "# Create eval_df\n",
    "eval_df = pd.DataFrame({\n",
    "    'personas': df['personas'],\n",
    "    'context': df['context'],\n",
    "    'act_response': df['act_response'],\n",
    "    'gen_response': response['gen_response'],\n",
    "    'response_time': response['response_time']\n",
    "})\n",
    "\n",
    "print(eval_df.isnull().sum())\n",
    "eval_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "device = 0 if torch.cuda.is_available() else -1  # device set to 0 for GPU, -1 for CPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n",
      "Device set to use cuda:0\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline, BertForSequenceClassification, BertTokenizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from nltk.corpus import stopwords\n",
    "from transformers import pipeline\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import gensim\n",
    "import sys\n",
    "import os\n",
    "\n",
    "\n",
    "# Add UniEval to PYTHONPATH\n",
    "sys.path.append(os.path.abspath(\"UniEval\"))  # Update with your actual path\n",
    "\n",
    "from UniEval.utils import convert_to_json\n",
    "from UniEval.metric.evaluator import get_evaluator\n",
    "\n",
    "\n",
    "# Lists to store the metrics\n",
    "ue_scores = []\n",
    "c_scores = []\n",
    "persona_distance_scores = []\n",
    "coh_unieval_scores = []\n",
    "\n",
    "\n",
    "bert_snli_dir = \"Fine-tuning/output/bert_snli\"\n",
    "bert_snli_model = BertForSequenceClassification.from_pretrained(bert_snli_dir)\n",
    "bert_snli_tokenizer = BertTokenizer.from_pretrained(bert_snli_dir)\n",
    "\n",
    "# Initialize the NLI pipeline for UE Score\n",
    "bert_on_snli = pipeline('text-classification', model = bert_snli_model, tokenizer = bert_snli_tokenizer, device=0)\n",
    "\n",
    "bert_dnli_dir = \"Fine-tuning/output/bert_dnli\"\n",
    "bert_dnli_model = BertForSequenceClassification.from_pretrained(bert_dnli_dir)\n",
    "bert_dnli_tokenizer = BertTokenizer.from_pretrained(bert_dnli_dir)\n",
    "\n",
    "# Initialize the NLI pipeline\n",
    "bert_on_dnli = pipeline('text-classification', model = bert_dnli_model, tokenizer = bert_dnli_tokenizer, device=0)\n",
    "\n",
    "\n",
    "# Initialize the Word2Vec Model\n",
    "word2vec_model = gensim.models.KeyedVectors.load_word2vec_format(\"./GoogleNews-vectors-negative300.bin\", binary=True)\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "\n",
    "\n",
    "def batch_calculate_c_scores(gen_responses, personas):\n",
    "    \"\"\"\n",
    "    Batched version of calculate_c_score using DNLI model.\n",
    "    \"\"\"\n",
    "    assert len(gen_responses) == len(personas), \"Mismatched input lengths\"\n",
    "\n",
    "    inputs = [f\"{p} {r}\" for p, r in zip(personas, gen_responses)]\n",
    "    results = bert_on_dnli(inputs)\n",
    "\n",
    "    label_mapping = {\n",
    "        'LABEL_0': 'negative',\n",
    "        'LABEL_1': 'neutral',\n",
    "        'LABEL_2': 'positive'\n",
    "    }\n",
    "\n",
    "    scores = []\n",
    "    for result in results:\n",
    "        label = label_mapping.get(result['label'], 'unknown')\n",
    "        if label == 'positive':\n",
    "            scores.append(1)\n",
    "        elif label == 'neutral':\n",
    "            scores.append(0)\n",
    "        elif label == 'negative':\n",
    "            scores.append(-1)\n",
    "        else:\n",
    "            scores.append(None)\n",
    "    return scores\n",
    "\n",
    "\n",
    "# def calculate_c_score(gen_response, persona):\n",
    "#     \"\"\"\n",
    "#     Calculate the C score based on the entailment results between a generated response (R)\n",
    "#     and a given persona (P).\n",
    "\n",
    "#     Returns:\n",
    "#     int: C-score with possible values:\n",
    "#          1 for entailment (positive),\n",
    "#          0 for neutral,\n",
    "#          -1 for contradiction (negative).\n",
    "#     \"\"\"\n",
    "\n",
    "#     # Define the label mapping to interpret the NLI model's output\n",
    "#     label_mapping = {\n",
    "#         'LABEL_0': 'negative',\n",
    "#         'LABEL_1': 'neutral',\n",
    "#         'LABEL_2': 'positive'\n",
    "#     }\n",
    "    \n",
    "#     # Check entailment between persona (P) and generated response (R)\n",
    "#     result_pr = bert_on_dnli(f\"{persona} {gen_response}\")\n",
    "#     label_pr = label_mapping.get(result_pr[0]['label'], 'unknown')\n",
    "\n",
    "#     # Determine C score based on entailment results\n",
    "#     if label_pr == 'positive':\n",
    "#         return 1\n",
    "#     elif label_pr == 'neutral':\n",
    "#         return 0\n",
    "#     elif label_pr == 'negative':\n",
    "#         return -1\n",
    "#     else:\n",
    "#         raise ValueError(f\"Unexpected label encountered: {label_pr}\")\n",
    "\n",
    "\n",
    "# def calculate_ue_score(act_response, gen_response, persona):\n",
    "#     \"\"\"\n",
    "#     Calculate the UE score based on entailment between persona, actual response, and generated response.\n",
    "\n",
    "#     Returns:\n",
    "#     int: UE score with possible values 2, 1, or 0.\n",
    "#     \"\"\"\n",
    "\n",
    "#     # Define the label mapping to interpret the NLI model's output\n",
    "#     label_mapping = {\n",
    "#         'LABEL_0': 'entailment',\n",
    "#         'LABEL_1': 'neutral',\n",
    "#         'LABEL_2': 'contradiction'\n",
    "#     }\n",
    "    \n",
    "#     # Check entailment between persona (P) and generated response (R)\n",
    "#     result_pr = bert_on_snli(f\"{persona} [SEP] {gen_response}\")\n",
    "#     label_pr = label_mapping.get(result_pr[0]['label'], 'unknown')\n",
    "\n",
    "#     # Check entailment between actual response (Q) and generated response (R)\n",
    "#     result_qr = bert_on_snli(f\"{act_response} [SEP] {gen_response}\")\n",
    "#     label_qr = label_mapping.get(result_qr[0]['label'], 'unknown')\n",
    "\n",
    "#     # Determine UE score based on entailment results\n",
    "#     if label_pr == 'entailment' and label_qr == 'entailment':\n",
    "#         return 2\n",
    "#     elif label_pr == 'entailment':\n",
    "#         return 1\n",
    "#     else:\n",
    "#         return 0\n",
    "\n",
    "\n",
    "def batch_calculate_ue_scores(act_responses, gen_responses, personas):\n",
    "    \"\"\"\n",
    "    Batched version of calculate_ue_score using SNLI model.\n",
    "    Returns a list of UE scores (0, 1, or 2).\n",
    "    \"\"\"\n",
    "    assert len(act_responses) == len(gen_responses) == len(personas), \"Mismatched lengths.\"\n",
    "\n",
    "    # Prepare NLI inputs\n",
    "    inputs_pr = [f\"{p} [SEP] {r}\" for p, r in zip(personas, gen_responses)]\n",
    "    inputs_qr = [f\"{q} [SEP] {r}\" for q, r in zip(act_responses, gen_responses)]\n",
    "\n",
    "    # Run both batches\n",
    "    results_pr = bert_on_snli(inputs_pr)\n",
    "    results_qr = bert_on_snli(inputs_qr)\n",
    "\n",
    "    label_mapping = {\n",
    "        'LABEL_0': 'entailment',\n",
    "        'LABEL_1': 'neutral',\n",
    "        'LABEL_2': 'contradiction'\n",
    "    }\n",
    "\n",
    "    scores = []\n",
    "    for res_pr, res_qr in zip(results_pr, results_qr):\n",
    "        label_pr = label_mapping.get(res_pr['label'], 'unknown')\n",
    "        label_qr = label_mapping.get(res_qr['label'], 'unknown')\n",
    "\n",
    "        if label_pr == 'entailment' and label_qr == 'entailment':\n",
    "            scores.append(2)\n",
    "        elif label_pr == 'entailment':\n",
    "            scores.append(1)\n",
    "        else:\n",
    "            scores.append(0)\n",
    "    return scores\n",
    "\n",
    "\n",
    "\n",
    "# def calculate_coh_unieval_score(personas, contexts, gen_responses):\n",
    "#     \"\"\"\n",
    "#     Args:\n",
    "#         personas (list): List of persona information as additional context.\n",
    "#         contexts (str or list): Conversation histories leading to the responses.\n",
    "#         gen_responses (str or list): Generated responses to be evaluated.\n",
    "\n",
    "#     Returns:\n",
    "#         float: The coherence score.\n",
    "#     \"\"\"\n",
    "    \n",
    "#     # Ensure personas is a list and flatten if necessary\n",
    "#     personas = [' '.join(p) if isinstance(p, list) else p for p in personas]\n",
    "\n",
    "#     # Ensure contexts and gen_responses are lists\n",
    "#     if isinstance(contexts, str):\n",
    "#         contexts = [contexts]  # Convert single string to list\n",
    "\n",
    "#     if isinstance(gen_responses, str):\n",
    "#         gen_responses = [gen_responses]  # Convert single string to list\n",
    "\n",
    "#     # Prepare inputs for UniEval\n",
    "#     data = convert_to_json(output_list=gen_responses, src_list=contexts, context_list=personas)\n",
    "\n",
    "#     # Initialize the evaluator for dialogue tasks\n",
    "#     evaluator = get_evaluator('dialogue')\n",
    "\n",
    "#     # Evaluate and obtain scores for all inputs\n",
    "#     eval_scores = evaluator.evaluate(data, print_result=False)\n",
    "    \n",
    "#     # Extract and return only the first coherence score\n",
    "#     return eval_scores[0].get(\"coherence\", None) if eval_scores else None\n",
    "\n",
    "\n",
    "def batch_calculate_coh_unieval_scores(personas_list, contexts_list, gen_responses_list):\n",
    "    \"\"\"\n",
    "    Batched coherence scoring using UniEval evaluator.\n",
    "    \"\"\"\n",
    "    # Ensure personas are joined as single strings\n",
    "    personas_list = [' '.join(p) if isinstance(p, list) else p for p in personas_list]\n",
    "    \n",
    "    # Convert to UniEval input format\n",
    "    data = convert_to_json(\n",
    "        output_list=gen_responses_list,\n",
    "        src_list=contexts_list,\n",
    "        context_list=personas_list\n",
    "    )\n",
    "    \n",
    "    # Load UniEval dialogue evaluator only once\n",
    "    evaluator = get_evaluator('dialogue')\n",
    "    eval_scores = evaluator.evaluate(data, print_result=False)\n",
    "\n",
    "    # Extract coherence scores\n",
    "    return [s.get(\"coherence\", None) for s in eval_scores]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def compute_persona_distance(persona, response, model, stop_words):\n",
    "    # Tokenize and filter stopwords\n",
    "    persona_tokens = [word for word in persona.lower().split() if word not in stop_words]\n",
    "    response_tokens = [word for word in response.lower().split() if word not in stop_words]\n",
    "    \n",
    "    # Get word vectors\n",
    "    persona_vecs = [model[word] for word in persona_tokens if word in model]\n",
    "    response_vecs = [model[word] for word in response_tokens if word in model]\n",
    "    \n",
    "    # If no vectors found, return zero similarity\n",
    "    if not persona_vecs or not response_vecs:\n",
    "        return 0.0\n",
    "    \n",
    "    # Compute average vectors\n",
    "    persona_avg_vec = np.mean(persona_vecs, axis=0)\n",
    "    response_avg_vec = np.mean(response_vecs, axis=0)\n",
    "    \n",
    "    # Compute cosine similarity\n",
    "    return cosine_similarity([persona_avg_vec], [response_avg_vec])[0][0]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "\n",
    "# Set the logging level to ERROR to suppress warnings about training\n",
    "logging.getLogger(\"transformers\").setLevel(logging.ERROR)\n",
    "\n",
    "# Default worst-case values\n",
    "worst_c_score = -1.0\n",
    "worst_ue_score = 0.0\n",
    "worst_persona_distance_score = 0.0\n",
    "worst_coh_unieval_score = 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating C Scores...\n",
      "Calculating UE Scores...\n",
      "Calculating UniEval Coherence Scores...\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "can only concatenate str (not \"float\") to str",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 21\u001b[0m\n\u001b[1;32m     18\u001b[0m ue_scores_batch \u001b[38;5;241m=\u001b[39m batch_calculate_ue_scores(valid_act_responses, valid_gen_responses, valid_personas)\n\u001b[1;32m     20\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCalculating UniEval Coherence Scores...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 21\u001b[0m coh_unieval_batch_scores \u001b[38;5;241m=\u001b[39m \u001b[43mbatch_calculate_coh_unieval_scores\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalid_personas\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalid_contexts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalid_gen_responses\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     23\u001b[0m \u001b[38;5;66;03m# Initialize all score lists with worst-case values\u001b[39;00m\n\u001b[1;32m     24\u001b[0m c_scores \u001b[38;5;241m=\u001b[39m [worst_c_score] \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mlen\u001b[39m(eval_df)\n",
      "Cell \u001b[0;32mIn[9], line 229\u001b[0m, in \u001b[0;36mbatch_calculate_coh_unieval_scores\u001b[0;34m(personas_list, contexts_list, gen_responses_list)\u001b[0m\n\u001b[1;32m    227\u001b[0m \u001b[38;5;66;03m# Load UniEval dialogue evaluator only once\u001b[39;00m\n\u001b[1;32m    228\u001b[0m evaluator \u001b[38;5;241m=\u001b[39m get_evaluator(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdialogue\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m--> 229\u001b[0m eval_scores \u001b[38;5;241m=\u001b[39m \u001b[43mevaluator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mevaluate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprint_result\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m    231\u001b[0m \u001b[38;5;66;03m# Extract coherence scores\u001b[39;00m\n\u001b[1;32m    232\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m [s\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcoherence\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;28;01mfor\u001b[39;00m s \u001b[38;5;129;01min\u001b[39;00m eval_scores]\n",
      "File \u001b[0;32m~/Desktop/DPO/UniEval/metric/evaluator.py:172\u001b[0m, in \u001b[0;36mDialogEvaluator.evaluate\u001b[0;34m(self, data, dims, overall, print_result)\u001b[0m\n\u001b[1;32m    170\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    171\u001b[0m             context_list\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m--> 172\u001b[0m     input_list \u001b[38;5;241m=\u001b[39m \u001b[43madd_question\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdimension\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdim\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_list\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m    173\u001b[0m \u001b[43m                              \u001b[49m\u001b[43msrc\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msrc_list\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcontext\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcontext_list\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtask\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    174\u001b[0m     score \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mscorer\u001b[38;5;241m.\u001b[39mscore(input_list)\n\u001b[1;32m    176\u001b[0m \u001b[38;5;66;03m# Please customize other dimensions here for summarization\u001b[39;00m\n\u001b[1;32m    177\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m~/Desktop/DPO/UniEval/utils.py:70\u001b[0m, in \u001b[0;36madd_question\u001b[0;34m(dimension, output, src, ref, context, task)\u001b[0m\n\u001b[1;32m     68\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m task \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdialogue\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m     69\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m dimension \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnaturalness\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[0;32m---> 70\u001b[0m         cur_input \u001b[38;5;241m=\u001b[39m \u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mquestion: Is this a natural response in the dialogue? </s> response: \u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43moutput\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\n\u001b[1;32m     71\u001b[0m     \u001b[38;5;28;01melif\u001b[39;00m dimension \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcoherence\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m     72\u001b[0m         cur_input \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mquestion: Is this a coherent response given the dialogue history? </s> response: \u001b[39m\u001b[38;5;124m'\u001b[39m\\\n\u001b[1;32m     73\u001b[0m                     \u001b[38;5;241m+\u001b[39m output[i] \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m </s> dialogue history: \u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m+\u001b[39m src[i]\n",
      "\u001b[0;31mTypeError\u001b[0m: can only concatenate str (not \"float\") to str"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "# Identify valid rows (non-null gen_response)\n",
    "valid_mask = eval_df['gen_response'].isna()\n",
    "valid_indices = eval_df[valid_mask].index.tolist()\n",
    "\n",
    "# Extract valid inputs\n",
    "valid_personas = eval_df.loc[valid_indices, 'personas'].tolist()\n",
    "valid_act_responses = eval_df.loc[valid_indices, 'act_response'].tolist()\n",
    "valid_contexts = eval_df.loc[valid_indices, 'context'].tolist()\n",
    "valid_gen_responses = eval_df.loc[valid_indices, 'gen_response'].tolist()\n",
    "\n",
    "# === Compute batch metrics with tqdm logging ===\n",
    "print(\"Calculating C Scores...\")\n",
    "c_scores_batch = batch_calculate_c_scores(valid_gen_responses, valid_personas)\n",
    "\n",
    "print(\"Calculating UE Scores...\")\n",
    "ue_scores_batch = batch_calculate_ue_scores(valid_act_responses, valid_gen_responses, valid_personas)\n",
    "\n",
    "print(\"Calculating UniEval Coherence Scores...\")\n",
    "coh_unieval_batch_scores = batch_calculate_coh_unieval_scores(valid_personas, valid_contexts, valid_gen_responses)\n",
    "\n",
    "# Initialize all score lists with worst-case values\n",
    "c_scores = [worst_c_score] * len(eval_df)\n",
    "ue_scores = [worst_ue_score] * len(eval_df)\n",
    "coh_unieval_scores = [worst_coh_unieval_score] * len(eval_df)\n",
    "\n",
    "# Fill valid indices with batch results using progress bar\n",
    "print(\"Filling final metric arrays...\")\n",
    "for i, c, ue, coh in tqdm(zip(valid_indices, c_scores_batch, ue_scores_batch, coh_unieval_batch_scores),\n",
    "                          total=len(valid_indices), desc=\"Populating Scores\"):\n",
    "    c_scores[i] = c\n",
    "    ue_scores[i] = ue\n",
    "    coh_unieval_scores[i] = coh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [00:00<00:00, 4467.53it/s]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Coh-UniEval</th>\n",
       "      <th>C Score</th>\n",
       "      <th>UE Score</th>\n",
       "      <th>Persona Distance</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.036880</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.454678</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.036696</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.353966</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.988755</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.452899</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>995</th>\n",
       "      <td>0.985394</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.350163</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>996</th>\n",
       "      <td>0.995642</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.498435</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>997</th>\n",
       "      <td>0.983637</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.265504</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>998</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>999</th>\n",
       "      <td>0.008397</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.138235</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1000 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     Coh-UniEval  C Score  UE Score  Persona Distance\n",
       "0       0.036880      0.0       0.0          0.454678\n",
       "1       0.036696      0.0       0.0          0.353966\n",
       "2       0.988755      0.0       0.0          0.452899\n",
       "3       0.000000     -1.0       0.0          0.000000\n",
       "4       0.000000     -1.0       0.0          0.000000\n",
       "..           ...      ...       ...               ...\n",
       "995     0.985394      0.0       0.0          0.350163\n",
       "996     0.995642      0.0       0.0          0.498435\n",
       "997     0.983637      1.0       1.0          0.265504\n",
       "998     0.000000     -1.0       0.0          0.000000\n",
       "999     0.008397      0.0       0.0          0.138235\n",
       "\n",
       "[1000 rows x 4 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Iterate over each row\n",
    "for index, row in tqdm(eval_df.iterrows(), total=len(eval_df)):\n",
    "    personas = row['personas']\n",
    "    contexts = row['context']\n",
    "    act_response = row['act_response']\n",
    "    gen_response = row['gen_response']\n",
    "    \n",
    "    # Check for NaN or None in gen_response\n",
    "    if pd.isna(gen_response):\n",
    "    \n",
    "        persona_distance_scores.append(worst_persona_distance_score)\n",
    "        \n",
    "        continue\n",
    "\n",
    "    persona_distance_scores.append(compute_persona_distance(personas, gen_response, word2vec_model,stop_words))\n",
    "\n",
    "\n",
    "# Compile metrics into DataFrame\n",
    "metrics_df = pd.DataFrame({\n",
    "    'Coh-UniEval': coh_unieval_scores,\n",
    "    'C Score': c_scores,\n",
    "    'UE Score': ue_scores,\n",
    "    'Persona Distance': persona_distance_scores\n",
    "})\n",
    "\n",
    "metrics_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Storing the full results\n",
    "output_path = f'./Metrics Results/{DATASET}/{LLM}{COT_}-results.xlsx'\n",
    "\n",
    "df_concat = pd.concat([eval_df, metrics_df], axis=1)\n",
    "\n",
    "df_concat.to_excel(output_path, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load and Aggregate Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>response_time</th>\n",
       "      <th>Coh-UniEval</th>\n",
       "      <th>C Score</th>\n",
       "      <th>UE Score</th>\n",
       "      <th>Persona Distance</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.905421</td>\n",
       "      <td>0.036880</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.454678</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.882345</td>\n",
       "      <td>0.036696</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.353966</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.565491</td>\n",
       "      <td>0.988755</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.452899</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.810716</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.868979</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>995</th>\n",
       "      <td>0.871568</td>\n",
       "      <td>0.985394</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.350163</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>996</th>\n",
       "      <td>0.871835</td>\n",
       "      <td>0.995642</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.498435</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>997</th>\n",
       "      <td>0.877787</td>\n",
       "      <td>0.983637</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.265504</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>998</th>\n",
       "      <td>0.880826</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>999</th>\n",
       "      <td>0.888007</td>\n",
       "      <td>0.008397</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.138235</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1000 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     response_time  Coh-UniEval  C Score  UE Score  Persona Distance\n",
       "0         0.905421     0.036880        0         0          0.454678\n",
       "1         0.882345     0.036696        0         0          0.353966\n",
       "2         0.565491     0.988755        0         0          0.452899\n",
       "3         0.810716     0.000000       -1         0          0.000000\n",
       "4         0.868979     0.000000       -1         0          0.000000\n",
       "..             ...          ...      ...       ...               ...\n",
       "995       0.871568     0.985394        0         0          0.350163\n",
       "996       0.871835     0.995642        0         0          0.498435\n",
       "997       0.877787     0.983637        1         1          0.265504\n",
       "998       0.880826     0.000000       -1         0          0.000000\n",
       "999       0.888007     0.008397        0         0          0.138235\n",
       "\n",
       "[1000 rows x 5 columns]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_excel(f\"Metrics Results/{DATASET}/{LLM}-results.xlsx\")\n",
    "metrics_df = df.drop(columns=[\"personas\", \"context\", \"act_response\",\"gen_response\"])\n",
    "metrics_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>response_time</th>\n",
       "      <th>Coh-UniEval</th>\n",
       "      <th>C Score</th>\n",
       "      <th>UE Score</th>\n",
       "      <th>Persona Distance</th>\n",
       "      <th>Failure Ratio</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Qwen2-5B-FoCus-length_prior</td>\n",
       "      <td>0.83 ± 0.12</td>\n",
       "      <td>0.48 ± 0.47</td>\n",
       "      <td>-0.18 ± 0.62</td>\n",
       "      <td>0.32 ± 0.68</td>\n",
       "      <td>0.27 ± 0.19</td>\n",
       "      <td>0.241 ± 0.00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                         Model response_time  Coh-UniEval       C Score  \\\n",
       "0  Qwen2-5B-FoCus-length_prior   0.83 ± 0.12  0.48 ± 0.47  -0.18 ± 0.62   \n",
       "\n",
       "      UE Score Persona Distance Failure Ratio  \n",
       "0  0.32 ± 0.68      0.27 ± 0.19  0.241 ± 0.00  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Calculate the mean (average) and standard deviation, rounded to 2 decimal places\n",
    "avg_values = metrics_df.mean().round(2)\n",
    "std_values = metrics_df.std(ddof=0).round(2)  # Use ddof=0 for population standard deviation\n",
    "\n",
    "# Combine the average and standard deviation into the format \"avg ± std\"\n",
    "combined_values = avg_values.astype(str) + \" ± \" + std_values.astype(str)\n",
    "\n",
    "# Insert the LLM name at the beginning of the combined values\n",
    "combined_values = combined_values.tolist()\n",
    "combined_values.insert(0, LLM)\n",
    "\n",
    "# Create a DataFrame for the combined average ± std row\n",
    "result_df = pd.DataFrame([combined_values], columns=['Model'] + metrics_df.columns.tolist())\n",
    "\n",
    "# Add the ratio of invalid gen_response\n",
    "invalid_gen_res_ratio = df['gen_response'].isna().sum() /len(df) \n",
    "\n",
    "result_df['Failure Ratio'] = f\"{round(invalid_gen_res_ratio, 3)} ± 0.00\"  # No std for Failure Ratio\n",
    "\n",
    "result_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>response_time</th>\n",
       "      <th>Coh-UniEval</th>\n",
       "      <th>C Score</th>\n",
       "      <th>UE Score</th>\n",
       "      <th>Persona Distance</th>\n",
       "      <th>Failure Ratio</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Qwen2-5B-Benchmark</td>\n",
       "      <td>0.77 ± 0.16</td>\n",
       "      <td>0.6 ± 0.49</td>\n",
       "      <td>-0.24 ± 0.76</td>\n",
       "      <td>0.14 ± 0.45</td>\n",
       "      <td>0.3 ± 0.26</td>\n",
       "      <td>0.39 ± 0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Qwen2-5B-DPO-AVG</td>\n",
       "      <td>0.85 ± 0.09</td>\n",
       "      <td>0.39 ± 0.48</td>\n",
       "      <td>-0.47 ± 0.76</td>\n",
       "      <td>0.16 ± 0.48</td>\n",
       "      <td>0.22 ± 0.28</td>\n",
       "      <td>0.595 ± 0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Qwen2-5B-DPO-LENGTH-PRIOR</td>\n",
       "      <td>0.6 ± 0.17</td>\n",
       "      <td>0.8 ± 0.39</td>\n",
       "      <td>0.03 ± 0.69</td>\n",
       "      <td>0.34 ± 0.67</td>\n",
       "      <td>0.4 ± 0.22</td>\n",
       "      <td>0.149 ± 0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Qwen2-5B-DPO</td>\n",
       "      <td>0.86 ± 0.09</td>\n",
       "      <td>0.39 ± 0.48</td>\n",
       "      <td>-0.47 ± 0.76</td>\n",
       "      <td>0.16 ± 0.48</td>\n",
       "      <td>0.22 ± 0.28</td>\n",
       "      <td>0.595 ± 0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Qwen2-5B-FoCus-length_prior</td>\n",
       "      <td>0.83 ± 0.12</td>\n",
       "      <td>0.48 ± 0.47</td>\n",
       "      <td>-0.18 ± 0.62</td>\n",
       "      <td>0.32 ± 0.68</td>\n",
       "      <td>0.27 ± 0.19</td>\n",
       "      <td>0.241 ± 0.00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                         Model response_time  Coh-UniEval       C Score  \\\n",
       "0           Qwen2-5B-Benchmark   0.77 ± 0.16   0.6 ± 0.49  -0.24 ± 0.76   \n",
       "1             Qwen2-5B-DPO-AVG   0.85 ± 0.09  0.39 ± 0.48  -0.47 ± 0.76   \n",
       "2    Qwen2-5B-DPO-LENGTH-PRIOR    0.6 ± 0.17   0.8 ± 0.39   0.03 ± 0.69   \n",
       "3                 Qwen2-5B-DPO   0.86 ± 0.09  0.39 ± 0.48  -0.47 ± 0.76   \n",
       "4  Qwen2-5B-FoCus-length_prior   0.83 ± 0.12  0.48 ± 0.47  -0.18 ± 0.62   \n",
       "\n",
       "      UE Score Persona Distance Failure Ratio  \n",
       "0  0.14 ± 0.45       0.3 ± 0.26   0.39 ± 0.00  \n",
       "1  0.16 ± 0.48      0.22 ± 0.28  0.595 ± 0.00  \n",
       "2  0.34 ± 0.67       0.4 ± 0.22  0.149 ± 0.00  \n",
       "3  0.16 ± 0.48      0.22 ± 0.28  0.595 ± 0.00  \n",
       "4  0.32 ± 0.68      0.27 ± 0.19  0.241 ± 0.00  "
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the existing Excel file and update or append the average row\n",
    "output_path = f'./Evaluations/{DATASET}{COT_}-results.xlsx'\n",
    "\n",
    "try:\n",
    "    # Load existing data\n",
    "    existing_df = pd.read_excel(output_path)\n",
    "    # Check if the model name already exists\n",
    "    if LLM in existing_df['Model'].values:\n",
    "        # Update the row with the same model name\n",
    "        existing_df.loc[existing_df['Model'] == LLM, :] = result_df.values\n",
    "    else:\n",
    "        # Append the new data\n",
    "        existing_df = pd.concat([existing_df, result_df], ignore_index=True)\n",
    "except FileNotFoundError:\n",
    "    # If the file does not exist, create a new DataFrame\n",
    "    existing_df = result_df\n",
    "\n",
    "# Save the updated DataFrame to an Excel file\n",
    "existing_df.to_excel(output_path, index=False)\n",
    "\n",
    "existing_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reviwing the Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>response_time</th>\n",
       "      <th>Coh-UniEval</th>\n",
       "      <th>C Score</th>\n",
       "      <th>UE Score</th>\n",
       "      <th>Persona Distance</th>\n",
       "      <th>Failure Ratio</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Qwen2-5B-Benchmark</td>\n",
       "      <td>0.77 ± 0.16</td>\n",
       "      <td>0.6 ± 0.49</td>\n",
       "      <td>-0.24 ± 0.76</td>\n",
       "      <td>0.14 ± 0.45</td>\n",
       "      <td>0.3 ± 0.26</td>\n",
       "      <td>0.39 ± 0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Qwen2-5B-DPO-AVG</td>\n",
       "      <td>0.85 ± 0.09</td>\n",
       "      <td>0.39 ± 0.48</td>\n",
       "      <td>-0.47 ± 0.76</td>\n",
       "      <td>0.16 ± 0.48</td>\n",
       "      <td>0.22 ± 0.28</td>\n",
       "      <td>0.595 ± 0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Qwen2-5B-DPO-LENGTH-PRIOR</td>\n",
       "      <td>0.6 ± 0.17</td>\n",
       "      <td>0.8 ± 0.39</td>\n",
       "      <td>0.03 ± 0.69</td>\n",
       "      <td>0.34 ± 0.67</td>\n",
       "      <td>0.4 ± 0.22</td>\n",
       "      <td>0.149 ± 0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Qwen2-5B-DPO</td>\n",
       "      <td>0.86 ± 0.09</td>\n",
       "      <td>0.39 ± 0.48</td>\n",
       "      <td>-0.47 ± 0.76</td>\n",
       "      <td>0.16 ± 0.48</td>\n",
       "      <td>0.22 ± 0.28</td>\n",
       "      <td>0.595 ± 0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Qwen2-5B-FoCus-length_prior</td>\n",
       "      <td>0.83 ± 0.12</td>\n",
       "      <td>0.48 ± 0.47</td>\n",
       "      <td>-0.18 ± 0.62</td>\n",
       "      <td>0.32 ± 0.68</td>\n",
       "      <td>0.27 ± 0.19</td>\n",
       "      <td>0.241 ± 0.00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                         Model response_time  Coh-UniEval       C Score  \\\n",
       "0           Qwen2-5B-Benchmark   0.77 ± 0.16   0.6 ± 0.49  -0.24 ± 0.76   \n",
       "1             Qwen2-5B-DPO-AVG   0.85 ± 0.09  0.39 ± 0.48  -0.47 ± 0.76   \n",
       "2    Qwen2-5B-DPO-LENGTH-PRIOR    0.6 ± 0.17   0.8 ± 0.39   0.03 ± 0.69   \n",
       "3                 Qwen2-5B-DPO   0.86 ± 0.09  0.39 ± 0.48  -0.47 ± 0.76   \n",
       "4  Qwen2-5B-FoCus-length_prior   0.83 ± 0.12  0.48 ± 0.47  -0.18 ± 0.62   \n",
       "\n",
       "      UE Score Persona Distance Failure Ratio  \n",
       "0  0.14 ± 0.45       0.3 ± 0.26   0.39 ± 0.00  \n",
       "1  0.16 ± 0.48      0.22 ± 0.28  0.595 ± 0.00  \n",
       "2  0.34 ± 0.67       0.4 ± 0.22  0.149 ± 0.00  \n",
       "3  0.16 ± 0.48      0.22 ± 0.28  0.595 ± 0.00  \n",
       "4  0.32 ± 0.68      0.27 ± 0.19  0.241 ± 0.00  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# DATASET = \"FoCus\"  \n",
    "# # COT_ = \"-COT\"\n",
    "# COT_ =  \"\"\n",
    "\n",
    "response = pd.read_excel(f'./Evaluations/{DATASET}{COT_}-results.xlsx')\n",
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
