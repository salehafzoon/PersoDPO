{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Installation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install gensim requests rouge bert_score openpyxl prettytable nltk gdown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import nltk\n",
    "# nltk.download('stopwords')\n",
    "# nltk.download('punkt')\n",
    "# nltk.download('punkt_tab')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import gdown\n",
    "# import gzip\n",
    "# import shutil\n",
    "# import os\n",
    "\n",
    "# compressed_path = \"./GoogleNews-vectors-negative300.bin.gz\"\n",
    "# decompressed_path = \"./GoogleNews-vectors-negative300.bin\"\n",
    "\n",
    "\n",
    "# # Google Drive file ID extracted from the link\n",
    "# file_id = \"0B7XkCwpI5KDYNlNUTTlSS21pQmM\"\n",
    "\n",
    "# # Download the file\n",
    "# gdown.download(f\"https://drive.google.com/uc?id={file_id}&export=download\", compressed_path, quiet=False)\n",
    "\n",
    "# print(\"Download completed.\")\n",
    "\n",
    "\n",
    "# # Extract the .bin file from .gz\n",
    "# with gzip.open(compressed_path, \"rb\") as f_in:\n",
    "#     with open(decompressed_path, \"wb\") as f_out:\n",
    "#         shutil.copyfileobj(f_in, f_out)\n",
    "\n",
    "# print(\"Extraction completed.\")\n",
    "\n",
    "# # (Optional) Delete the .gz file after extraction\n",
    "# os.remove(compressed_path)\n",
    "# print(\"Deleted the compressed file.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Performance Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import warnings\n",
    "import logging\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "logging.getLogger('transformers').setLevel(logging.ERROR)\n",
    "\n",
    "# Set the logging level to ERROR to ignore warnings\n",
    "logging.getLogger(\"transformers\").setLevel(logging.ERROR)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DATASET = \"Blended Skill Talk\"                                             \n",
    "DATASET = \"IT-ConvAI2\"                                             \n",
    "# DATASET = \"FoCus\"                                             \n",
    "\n",
    "# LLM = \"Qwen2-5B-Benchmark\"     \n",
    "# LLM = \"Qwen2-5B-DPO\"    \n",
    "# LLM = \"Qwen2-5B-DPO-AVG\"     \n",
    "# LLM = \"Qwen2-5B-DPO-LENGTH-PRIOR\"   \n",
    "\n",
    "\n",
    "LLM = \"Qwen2-7B-Instruct\"\n",
    "# LLM = \"Mistral-7B-Instruct\"\n",
    "# LLM = \"Llama3-1-8B-Instruct\"\n",
    "                              \n",
    "COT_SETUP = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape: (1183, 3)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>personas</th>\n",
       "      <th>context</th>\n",
       "      <th>act_response</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>i love disneyland and mickey mouse.i love to s...</td>\n",
       "      <td>User1: no , we recently purchased a new house ...</td>\n",
       "      <td>User2: yes i love mickey mouse such a cute lit...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>i love to drink fancy tea.i have a big library...</td>\n",
       "      <td>User1: hi how are you doing ? i am okay how ab...</td>\n",
       "      <td>User2: i am doing good . just sipping tea . wh...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            personas  \\\n",
       "0  i love disneyland and mickey mouse.i love to s...   \n",
       "1  i love to drink fancy tea.i have a big library...   \n",
       "\n",
       "                                             context  \\\n",
       "0  User1: no , we recently purchased a new house ...   \n",
       "1  User1: hi how are you doing ? i am okay how ab...   \n",
       "\n",
       "                                        act_response  \n",
       "0  User2: yes i love mickey mouse such a cute lit...  \n",
       "1  User2: i am doing good . just sipping tea . wh...  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(f'./Prompts/{DATASET}.csv')\n",
    "print(\"Shape:\", df.shape)\n",
    "\n",
    "df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "personas        0\n",
      "context         0\n",
      "act_response    0\n",
      "dtype: int64\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>personas</th>\n",
       "      <th>context</th>\n",
       "      <th>act_response</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>i love disneyland and mickey mouse.i love to s...</td>\n",
       "      <td>User1: no , we recently purchased a new house ...</td>\n",
       "      <td>yes i love mickey mouse such a cute little rat</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>i love to drink fancy tea.i have a big library...</td>\n",
       "      <td>User1: hi how are you doing ? i am okay how ab...</td>\n",
       "      <td>i am doing good . just sipping tea . what do y...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            personas  \\\n",
       "0  i love disneyland and mickey mouse.i love to s...   \n",
       "1  i love to drink fancy tea.i have a big library...   \n",
       "\n",
       "                                             context  \\\n",
       "0  User1: no , we recently purchased a new house ...   \n",
       "1  User1: hi how are you doing ? i am okay how ab...   \n",
       "\n",
       "                                        act_response  \n",
       "0     yes i love mickey mouse such a cute little rat  \n",
       "1  i am doing good . just sipping tea . what do y...  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Only For: FoCus, IT-ConvAI2\n",
    "\n",
    "df['act_response'] = df['act_response'].apply(lambda x: x.split(':', 1)[1].strip() if ':' in x else x.strip())\n",
    "\n",
    "print(df.isnull().sum())\n",
    "df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape: (1183, 2)\n",
      "\n",
      "Missing Values:\n",
      "gen_response     22\n",
      "response_time     0\n",
      "dtype: int64\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>gen_response</th>\n",
       "      <th>response_time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>I'm sorry to hear about your new house purchas...</td>\n",
       "      <td>1.600242</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>I'm doing well, thank you for asking. I'm just...</td>\n",
       "      <td>1.664674</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>My name is Cloudy, and I'm a little girl with ...</td>\n",
       "      <td>1.387386</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Yes, I love turtles! They're so cute and fasci...</td>\n",
       "      <td>1.314589</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>My cats' names are Whiskers and Mittens. They'...</td>\n",
       "      <td>1.070947</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1178</th>\n",
       "      <td>Oh, interesting! I'm actually a musician. I cr...</td>\n",
       "      <td>1.278092</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1179</th>\n",
       "      <td>Scorpions are actually neither insects nor ani...</td>\n",
       "      <td>3.845814</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1180</th>\n",
       "      <td>I work as a custodian to help pay the bills, b...</td>\n",
       "      <td>1.871604</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1181</th>\n",
       "      <td>I'm a vegan who loves hummus, roller coasters,...</td>\n",
       "      <td>2.051421</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1182</th>\n",
       "      <td>I love trying out new recipes, especially vega...</td>\n",
       "      <td>2.400737</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1183 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           gen_response  response_time\n",
       "0     I'm sorry to hear about your new house purchas...       1.600242\n",
       "1     I'm doing well, thank you for asking. I'm just...       1.664674\n",
       "2     My name is Cloudy, and I'm a little girl with ...       1.387386\n",
       "3     Yes, I love turtles! They're so cute and fasci...       1.314589\n",
       "4     My cats' names are Whiskers and Mittens. They'...       1.070947\n",
       "...                                                 ...            ...\n",
       "1178  Oh, interesting! I'm actually a musician. I cr...       1.278092\n",
       "1179  Scorpions are actually neither insects nor ani...       3.845814\n",
       "1180  I work as a custodian to help pay the bills, b...       1.871604\n",
       "1181  I'm a vegan who loves hummus, roller coasters,...       2.051421\n",
       "1182  I love trying out new recipes, especially vega...       2.400737\n",
       "\n",
       "[1183 rows x 2 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "COT_ = \"-COT\" if COT_SETUP else \"\"\n",
    "\n",
    "response = pd.read_csv(f'Responses/{DATASET}/{LLM}{COT_}.csv')\n",
    "print(\"Shape:\", response.shape)\n",
    "\n",
    "print(\"\\nMissing Values:\")\n",
    "print(response.isnull().sum())\n",
    "\n",
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Maximum Response Length (in words): 88\n"
     ]
    }
   ],
   "source": [
    "# Calculate maximum number of words in each column\n",
    "max_response_length = response['gen_response'].dropna().apply(lambda x: len(x.split())).max()\n",
    "\n",
    "print(f\"Maximum Response Length (in words): {max_response_length}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "personas          0\n",
      "context           0\n",
      "act_response      0\n",
      "gen_response     22\n",
      "response_time     0\n",
      "dtype: int64\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>personas</th>\n",
       "      <th>context</th>\n",
       "      <th>act_response</th>\n",
       "      <th>gen_response</th>\n",
       "      <th>response_time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>i love disneyland and mickey mouse.i love to s...</td>\n",
       "      <td>User1: no , we recently purchased a new house ...</td>\n",
       "      <td>yes i love mickey mouse such a cute little rat</td>\n",
       "      <td>I'm sorry to hear about your new house purchas...</td>\n",
       "      <td>1.600242</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>i love to drink fancy tea.i have a big library...</td>\n",
       "      <td>User1: hi how are you doing ? i am okay how ab...</td>\n",
       "      <td>i am doing good . just sipping tea . what do y...</td>\n",
       "      <td>I'm doing well, thank you for asking. I'm just...</td>\n",
       "      <td>1.664674</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>im a little girl.ive superpowers.i like to mak...</td>\n",
       "      <td>User1: what is your name ? are you a male or f...</td>\n",
       "      <td>im a girl . i do not give out my name .</td>\n",
       "      <td>My name is Cloudy, and I'm a little girl with ...</td>\n",
       "      <td>1.387386</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>i love cats and have two cats.my favorite seas...</td>\n",
       "      <td>User1: hi ! do you like turtles ?</td>\n",
       "      <td>i am much more of a cat person actually</td>\n",
       "      <td>Yes, I love turtles! They're so cute and fasci...</td>\n",
       "      <td>1.314589</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>i love cats and have two cats.my favorite seas...</td>\n",
       "      <td>User1: what are your kitties names ?</td>\n",
       "      <td>snow and winter , named after my favorite season</td>\n",
       "      <td>My cats' names are Whiskers and Mittens. They'...</td>\n",
       "      <td>1.070947</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            personas  \\\n",
       "0  i love disneyland and mickey mouse.i love to s...   \n",
       "1  i love to drink fancy tea.i have a big library...   \n",
       "2  im a little girl.ive superpowers.i like to mak...   \n",
       "3  i love cats and have two cats.my favorite seas...   \n",
       "4  i love cats and have two cats.my favorite seas...   \n",
       "\n",
       "                                             context  \\\n",
       "0  User1: no , we recently purchased a new house ...   \n",
       "1  User1: hi how are you doing ? i am okay how ab...   \n",
       "2  User1: what is your name ? are you a male or f...   \n",
       "3                  User1: hi ! do you like turtles ?   \n",
       "4               User1: what are your kitties names ?   \n",
       "\n",
       "                                        act_response  \\\n",
       "0     yes i love mickey mouse such a cute little rat   \n",
       "1  i am doing good . just sipping tea . what do y...   \n",
       "2            im a girl . i do not give out my name .   \n",
       "3            i am much more of a cat person actually   \n",
       "4   snow and winter , named after my favorite season   \n",
       "\n",
       "                                        gen_response  response_time  \n",
       "0  I'm sorry to hear about your new house purchas...       1.600242  \n",
       "1  I'm doing well, thank you for asking. I'm just...       1.664674  \n",
       "2  My name is Cloudy, and I'm a little girl with ...       1.387386  \n",
       "3  Yes, I love turtles! They're so cute and fasci...       1.314589  \n",
       "4  My cats' names are Whiskers and Mittens. They'...       1.070947  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import string\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# Initialize stop words\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "# Function to preprocess text\n",
    "def preprocess_text(text, remove_stop_words=True):\n",
    "    if pd.isnull(text):\n",
    "        return None\n",
    "    text = text.lower()  # Lowercasing\n",
    "    text = text.translate(str.maketrans('', '', string.punctuation))  # Removing punctuation\n",
    "    tokens = word_tokenize(text)  # Tokenization\n",
    "    if remove_stop_words:\n",
    "        tokens = [word for word in tokens if word not in stop_words]  # Removing stop words\n",
    "    return ' '.join(tokens)  # Join tokens back into a single string\n",
    "\n",
    "\n",
    "# Create eval_df\n",
    "eval_df = pd.DataFrame({\n",
    "    'personas': df['personas'],\n",
    "    'context': df['context'],\n",
    "    'act_response': df['act_response'],\n",
    "    'gen_response': response['gen_response'],\n",
    "    'response_time': response['response_time']\n",
    "})\n",
    "\n",
    "print(eval_df.isnull().sum())\n",
    "eval_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "device = 0 if torch.cuda.is_available() else -1  # device set to 0 for GPU, -1 for CPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n",
      "Device set to use cuda:0\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline, BertForSequenceClassification, BertTokenizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from nltk.corpus import stopwords\n",
    "from transformers import pipeline\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import gensim\n",
    "import sys\n",
    "import os\n",
    "\n",
    "\n",
    "# Add UniEval to PYTHONPATH\n",
    "sys.path.append(os.path.abspath(\"UniEval\"))  # Update with your actual path\n",
    "\n",
    "from UniEval.utils import convert_to_json\n",
    "from UniEval.metric.evaluator import get_evaluator\n",
    "\n",
    "\n",
    "# Lists to store the metrics\n",
    "ue_scores = []\n",
    "c_scores = []\n",
    "persona_distance_scores = []\n",
    "coh_unieval_scores = []\n",
    "\n",
    "\n",
    "bert_snli_dir = \"Fine-tuning/output/bert_snli\"\n",
    "bert_snli_model = BertForSequenceClassification.from_pretrained(bert_snli_dir)\n",
    "bert_snli_tokenizer = BertTokenizer.from_pretrained(bert_snli_dir)\n",
    "\n",
    "# Initialize the NLI pipeline for UE Score\n",
    "bert_on_snli = pipeline('text-classification', model = bert_snli_model, tokenizer = bert_snli_tokenizer, device=0)\n",
    "\n",
    "bert_dnli_dir = \"Fine-tuning/output/bert_dnli\"\n",
    "bert_dnli_model = BertForSequenceClassification.from_pretrained(bert_dnli_dir)\n",
    "bert_dnli_tokenizer = BertTokenizer.from_pretrained(bert_dnli_dir)\n",
    "\n",
    "# Initialize the NLI pipeline\n",
    "bert_on_dnli = pipeline('text-classification', model = bert_dnli_model, tokenizer = bert_dnli_tokenizer, device=0)\n",
    "\n",
    "\n",
    "# Initialize the Word2Vec Model\n",
    "word2vec_model = gensim.models.KeyedVectors.load_word2vec_format(\"./GoogleNews-vectors-negative300.bin\", binary=True)\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "\n",
    "\n",
    "def batch_calculate_c_scores(gen_responses, personas):\n",
    "    \"\"\"\n",
    "    Batched version of calculate_c_score using DNLI model.\n",
    "    \"\"\"\n",
    "    assert len(gen_responses) == len(personas), \"Mismatched input lengths\"\n",
    "\n",
    "    inputs = [f\"{p} {r}\" for p, r in zip(personas, gen_responses)]\n",
    "    results = bert_on_dnli(inputs)\n",
    "\n",
    "    label_mapping = {\n",
    "        'LABEL_0': 'negative',\n",
    "        'LABEL_1': 'neutral',\n",
    "        'LABEL_2': 'positive'\n",
    "    }\n",
    "\n",
    "    scores = []\n",
    "    for result in results:\n",
    "        label = label_mapping.get(result['label'], 'unknown')\n",
    "        if label == 'positive':\n",
    "            scores.append(1)\n",
    "        elif label == 'neutral':\n",
    "            scores.append(0)\n",
    "        elif label == 'negative':\n",
    "            scores.append(-1)\n",
    "        else:\n",
    "            scores.append(None)\n",
    "    return scores\n",
    "\n",
    "\n",
    "# def calculate_c_score(gen_response, persona):\n",
    "#     \"\"\"\n",
    "#     Calculate the C score based on the entailment results between a generated response (R)\n",
    "#     and a given persona (P).\n",
    "\n",
    "#     Returns:\n",
    "#     int: C-score with possible values:\n",
    "#          1 for entailment (positive),\n",
    "#          0 for neutral,\n",
    "#          -1 for contradiction (negative).\n",
    "#     \"\"\"\n",
    "\n",
    "#     # Define the label mapping to interpret the NLI model's output\n",
    "#     label_mapping = {\n",
    "#         'LABEL_0': 'negative',\n",
    "#         'LABEL_1': 'neutral',\n",
    "#         'LABEL_2': 'positive'\n",
    "#     }\n",
    "    \n",
    "#     # Check entailment between persona (P) and generated response (R)\n",
    "#     result_pr = bert_on_dnli(f\"{persona} {gen_response}\")\n",
    "#     label_pr = label_mapping.get(result_pr[0]['label'], 'unknown')\n",
    "\n",
    "#     # Determine C score based on entailment results\n",
    "#     if label_pr == 'positive':\n",
    "#         return 1\n",
    "#     elif label_pr == 'neutral':\n",
    "#         return 0\n",
    "#     elif label_pr == 'negative':\n",
    "#         return -1\n",
    "#     else:\n",
    "#         raise ValueError(f\"Unexpected label encountered: {label_pr}\")\n",
    "\n",
    "\n",
    "# def calculate_ue_score(act_response, gen_response, persona):\n",
    "#     \"\"\"\n",
    "#     Calculate the UE score based on entailment between persona, actual response, and generated response.\n",
    "\n",
    "#     Returns:\n",
    "#     int: UE score with possible values 2, 1, or 0.\n",
    "#     \"\"\"\n",
    "\n",
    "#     # Define the label mapping to interpret the NLI model's output\n",
    "#     label_mapping = {\n",
    "#         'LABEL_0': 'entailment',\n",
    "#         'LABEL_1': 'neutral',\n",
    "#         'LABEL_2': 'contradiction'\n",
    "#     }\n",
    "    \n",
    "#     # Check entailment between persona (P) and generated response (R)\n",
    "#     result_pr = bert_on_snli(f\"{persona} [SEP] {gen_response}\")\n",
    "#     label_pr = label_mapping.get(result_pr[0]['label'], 'unknown')\n",
    "\n",
    "#     # Check entailment between actual response (Q) and generated response (R)\n",
    "#     result_qr = bert_on_snli(f\"{act_response} [SEP] {gen_response}\")\n",
    "#     label_qr = label_mapping.get(result_qr[0]['label'], 'unknown')\n",
    "\n",
    "#     # Determine UE score based on entailment results\n",
    "#     if label_pr == 'entailment' and label_qr == 'entailment':\n",
    "#         return 2\n",
    "#     elif label_pr == 'entailment':\n",
    "#         return 1\n",
    "#     else:\n",
    "#         return 0\n",
    "\n",
    "\n",
    "def batch_calculate_ue_scores(act_responses, gen_responses, personas):\n",
    "    \"\"\"\n",
    "    Batched version of calculate_ue_score using SNLI model.\n",
    "    Returns a list of UE scores (0, 1, or 2).\n",
    "    \"\"\"\n",
    "    assert len(act_responses) == len(gen_responses) == len(personas), \"Mismatched lengths.\"\n",
    "\n",
    "    # Prepare NLI inputs\n",
    "    inputs_pr = [f\"{p} [SEP] {r}\" for p, r in zip(personas, gen_responses)]\n",
    "    inputs_qr = [f\"{q} [SEP] {r}\" for q, r in zip(act_responses, gen_responses)]\n",
    "\n",
    "    # Run both batches\n",
    "    results_pr = bert_on_snli(inputs_pr)\n",
    "    results_qr = bert_on_snli(inputs_qr)\n",
    "\n",
    "    label_mapping = {\n",
    "        'LABEL_0': 'entailment',\n",
    "        'LABEL_1': 'neutral',\n",
    "        'LABEL_2': 'contradiction'\n",
    "    }\n",
    "\n",
    "    scores = []\n",
    "    for res_pr, res_qr in zip(results_pr, results_qr):\n",
    "        label_pr = label_mapping.get(res_pr['label'], 'unknown')\n",
    "        label_qr = label_mapping.get(res_qr['label'], 'unknown')\n",
    "\n",
    "        if label_pr == 'entailment' and label_qr == 'entailment':\n",
    "            scores.append(2)\n",
    "        elif label_pr == 'entailment':\n",
    "            scores.append(1)\n",
    "        else:\n",
    "            scores.append(0)\n",
    "    return scores\n",
    "\n",
    "\n",
    "\n",
    "# def calculate_coh_unieval_score(personas, contexts, gen_responses):\n",
    "#     \"\"\"\n",
    "#     Args:\n",
    "#         personas (list): List of persona information as additional context.\n",
    "#         contexts (str or list): Conversation histories leading to the responses.\n",
    "#         gen_responses (str or list): Generated responses to be evaluated.\n",
    "\n",
    "#     Returns:\n",
    "#         float: The coherence score.\n",
    "#     \"\"\"\n",
    "    \n",
    "#     # Ensure personas is a list and flatten if necessary\n",
    "#     personas = [' '.join(p) if isinstance(p, list) else p for p in personas]\n",
    "\n",
    "#     # Ensure contexts and gen_responses are lists\n",
    "#     if isinstance(contexts, str):\n",
    "#         contexts = [contexts]  # Convert single string to list\n",
    "\n",
    "#     if isinstance(gen_responses, str):\n",
    "#         gen_responses = [gen_responses]  # Convert single string to list\n",
    "\n",
    "#     # Prepare inputs for UniEval\n",
    "#     data = convert_to_json(output_list=gen_responses, src_list=contexts, context_list=personas)\n",
    "\n",
    "#     # Initialize the evaluator for dialogue tasks\n",
    "#     evaluator = get_evaluator('dialogue')\n",
    "\n",
    "#     # Evaluate and obtain scores for all inputs\n",
    "#     eval_scores = evaluator.evaluate(data, print_result=False)\n",
    "    \n",
    "#     # Extract and return only the first coherence score\n",
    "#     return eval_scores[0].get(\"coherence\", None) if eval_scores else None\n",
    "\n",
    "\n",
    "def batch_calculate_coh_unieval_scores(personas_list, contexts_list, gen_responses_list):\n",
    "    \"\"\"\n",
    "    Batched coherence scoring using UniEval evaluator.\n",
    "    \"\"\"\n",
    "    # Ensure personas are joined as single strings\n",
    "    personas_list = [' '.join(p) if isinstance(p, list) else p for p in personas_list]\n",
    "    \n",
    "    # Convert to UniEval input format\n",
    "    data = convert_to_json(\n",
    "        output_list=gen_responses_list,\n",
    "        src_list=contexts_list,\n",
    "        context_list=personas_list\n",
    "    )\n",
    "    \n",
    "    # Load UniEval dialogue evaluator only once\n",
    "    evaluator = get_evaluator('dialogue')\n",
    "    eval_scores = evaluator.evaluate(data, print_result=False)\n",
    "\n",
    "    # Extract coherence scores\n",
    "    return [s.get(\"coherence\", None) for s in eval_scores]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def compute_persona_distance(persona, response, model, stop_words):\n",
    "    # Tokenize and filter stopwords\n",
    "    persona_tokens = [word for word in persona.lower().split() if word not in stop_words]\n",
    "    response_tokens = [word for word in response.lower().split() if word not in stop_words]\n",
    "    \n",
    "    # Get word vectors\n",
    "    persona_vecs = [model[word] for word in persona_tokens if word in model]\n",
    "    response_vecs = [model[word] for word in response_tokens if word in model]\n",
    "    \n",
    "    # If no vectors found, return zero similarity\n",
    "    if not persona_vecs or not response_vecs:\n",
    "        return 0.0\n",
    "    \n",
    "    # Compute average vectors\n",
    "    persona_avg_vec = np.mean(persona_vecs, axis=0)\n",
    "    response_avg_vec = np.mean(response_vecs, axis=0)\n",
    "    \n",
    "    # Compute cosine similarity\n",
    "    return cosine_similarity([persona_avg_vec], [response_avg_vec])[0][0]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "\n",
    "# Set the logging level to ERROR to suppress warnings about training\n",
    "logging.getLogger(\"transformers\").setLevel(logging.ERROR)\n",
    "\n",
    "# Default worst-case values\n",
    "worst_c_score = -1.0\n",
    "worst_ue_score = 0.0\n",
    "worst_persona_distance_score = 0.0\n",
    "worst_coh_unieval_score = 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating C Scores...\n",
      "Calculating UE Scores...\n",
      "Calculating UniEval Coherence Scores...\n",
      "Filling final metric arrays...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Populating Scores: 100%|██████████| 1161/1161 [00:00<00:00, 3164124.07it/s]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "# Identify valid rows (non-null gen_response)\n",
    "valid_mask = ~eval_df['gen_response'].isna()\n",
    "valid_indices = eval_df[valid_mask].index.tolist()\n",
    "\n",
    "# Extract valid inputs\n",
    "valid_personas = eval_df.loc[valid_indices, 'personas'].tolist()\n",
    "valid_act_responses = eval_df.loc[valid_indices, 'act_response'].tolist()\n",
    "valid_contexts = eval_df.loc[valid_indices, 'context'].tolist()\n",
    "valid_gen_responses = eval_df.loc[valid_indices, 'gen_response'].tolist()\n",
    "\n",
    "# === Compute batch metrics with tqdm logging ===\n",
    "print(\"Calculating C Scores...\")\n",
    "c_scores_batch = batch_calculate_c_scores(valid_gen_responses, valid_personas)\n",
    "\n",
    "print(\"Calculating UE Scores...\")\n",
    "ue_scores_batch = batch_calculate_ue_scores(valid_act_responses, valid_gen_responses, valid_personas)\n",
    "\n",
    "print(\"Calculating UniEval Coherence Scores...\")\n",
    "coh_unieval_batch_scores = batch_calculate_coh_unieval_scores(valid_personas, valid_contexts, valid_gen_responses)\n",
    "\n",
    "# Initialize all score lists with worst-case values\n",
    "c_scores = [worst_c_score] * len(eval_df)\n",
    "ue_scores = [worst_ue_score] * len(eval_df)\n",
    "coh_unieval_scores = [worst_coh_unieval_score] * len(eval_df)\n",
    "\n",
    "# Fill valid indices with batch results using progress bar\n",
    "print(\"Filling final metric arrays...\")\n",
    "for i, c, ue, coh in tqdm(zip(valid_indices, c_scores_batch, ue_scores_batch, coh_unieval_batch_scores),\n",
    "                          total=len(valid_indices), desc=\"Populating Scores\"):\n",
    "    c_scores[i] = c\n",
    "    ue_scores[i] = ue\n",
    "    coh_unieval_scores[i] = coh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1183/1183 [00:00<00:00, 3432.06it/s]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Coh-UniEval</th>\n",
       "      <th>C Score</th>\n",
       "      <th>UE Score</th>\n",
       "      <th>Persona Distance</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.984436</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.586262</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.989371</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.514717</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.284176</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.708235</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.997843</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.637448</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.995969</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.419571</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1178</th>\n",
       "      <td>0.994208</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.611310</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1179</th>\n",
       "      <td>0.993923</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.584755</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1180</th>\n",
       "      <td>0.997008</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.845741</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1181</th>\n",
       "      <td>0.999340</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.769901</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1182</th>\n",
       "      <td>0.827272</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.857344</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1183 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      Coh-UniEval  C Score  UE Score  Persona Distance\n",
       "0        0.984436      0.0       0.0          0.586262\n",
       "1        0.989371      0.0       0.0          0.514717\n",
       "2        0.284176      1.0       2.0          0.708235\n",
       "3        0.997843      0.0       0.0          0.637448\n",
       "4        0.995969      1.0       0.0          0.419571\n",
       "...           ...      ...       ...               ...\n",
       "1178     0.994208     -1.0       0.0          0.611310\n",
       "1179     0.993923      1.0       2.0          0.584755\n",
       "1180     0.997008      1.0       0.0          0.845741\n",
       "1181     0.999340      0.0       1.0          0.769901\n",
       "1182     0.827272      1.0       0.0          0.857344\n",
       "\n",
       "[1183 rows x 4 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Iterate over each row\n",
    "for index, row in tqdm(eval_df.iterrows(), total=len(eval_df)):\n",
    "    personas = row['personas']\n",
    "    contexts = row['context']\n",
    "    act_response = row['act_response']\n",
    "    gen_response = row['gen_response']\n",
    "    \n",
    "    # Check for NaN or None in gen_response\n",
    "    if pd.isna(gen_response):\n",
    "    \n",
    "        persona_distance_scores.append(worst_persona_distance_score)\n",
    "        \n",
    "        continue\n",
    "\n",
    "    persona_distance_scores.append(compute_persona_distance(personas, gen_response, word2vec_model,stop_words))\n",
    "\n",
    "\n",
    "# Compile metrics into DataFrame\n",
    "metrics_df = pd.DataFrame({\n",
    "    'Coh-UniEval': coh_unieval_scores,\n",
    "    'C Score': c_scores,\n",
    "    'UE Score': ue_scores,\n",
    "    'Persona Distance': persona_distance_scores\n",
    "})\n",
    "\n",
    "metrics_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Storing the full results\n",
    "output_path = f'./Metrics Results/{DATASET}/{LLM}{COT_}-results.xlsx'\n",
    "\n",
    "df_concat = pd.concat([eval_df, metrics_df], axis=1)\n",
    "\n",
    "df_concat.to_excel(output_path, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load and Aggregate Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>response_time</th>\n",
       "      <th>Coh-UniEval</th>\n",
       "      <th>C Score</th>\n",
       "      <th>UE Score</th>\n",
       "      <th>Persona Distance</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.600242</td>\n",
       "      <td>0.984436</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.586262</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.664674</td>\n",
       "      <td>0.989371</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.514717</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.387386</td>\n",
       "      <td>0.284176</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0.708235</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.314589</td>\n",
       "      <td>0.997843</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.637448</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.070947</td>\n",
       "      <td>0.995969</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.419571</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1178</th>\n",
       "      <td>1.278092</td>\n",
       "      <td>0.994208</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.611310</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1179</th>\n",
       "      <td>3.845814</td>\n",
       "      <td>0.993923</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0.584755</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1180</th>\n",
       "      <td>1.871604</td>\n",
       "      <td>0.997008</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.845741</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1181</th>\n",
       "      <td>2.051421</td>\n",
       "      <td>0.999340</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.769901</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1182</th>\n",
       "      <td>2.400737</td>\n",
       "      <td>0.827272</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.857344</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1183 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      response_time  Coh-UniEval  C Score  UE Score  Persona Distance\n",
       "0          1.600242     0.984436        0         0          0.586262\n",
       "1          1.664674     0.989371        0         0          0.514717\n",
       "2          1.387386     0.284176        1         2          0.708235\n",
       "3          1.314589     0.997843        0         0          0.637448\n",
       "4          1.070947     0.995969        1         0          0.419571\n",
       "...             ...          ...      ...       ...               ...\n",
       "1178       1.278092     0.994208       -1         0          0.611310\n",
       "1179       3.845814     0.993923        1         2          0.584755\n",
       "1180       1.871604     0.997008        1         0          0.845741\n",
       "1181       2.051421     0.999340        0         1          0.769901\n",
       "1182       2.400737     0.827272        1         0          0.857344\n",
       "\n",
       "[1183 rows x 5 columns]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_excel(f\"Metrics Results/{DATASET}/{LLM}-results.xlsx\")\n",
    "metrics_df = df.drop(columns=[\"personas\", \"context\", \"act_response\",\"gen_response\"])\n",
    "metrics_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>response_time</th>\n",
       "      <th>Coh-UniEval</th>\n",
       "      <th>C Score</th>\n",
       "      <th>UE Score</th>\n",
       "      <th>Persona Distance</th>\n",
       "      <th>Failure Ratio</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Qwen2-7B-Instruct</td>\n",
       "      <td>1.97 ± 0.72</td>\n",
       "      <td>0.89 ± 0.24</td>\n",
       "      <td>0.23 ± 0.59</td>\n",
       "      <td>0.42 ± 0.73</td>\n",
       "      <td>0.61 ± 0.17</td>\n",
       "      <td>0.019 ± 0.00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               Model response_time  Coh-UniEval      C Score     UE Score  \\\n",
       "0  Qwen2-7B-Instruct   1.97 ± 0.72  0.89 ± 0.24  0.23 ± 0.59  0.42 ± 0.73   \n",
       "\n",
       "  Persona Distance Failure Ratio  \n",
       "0      0.61 ± 0.17  0.019 ± 0.00  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Calculate the mean (average) and standard deviation, rounded to 2 decimal places\n",
    "avg_values = metrics_df.mean().round(2)\n",
    "std_values = metrics_df.std(ddof=0).round(2)  # Use ddof=0 for population standard deviation\n",
    "\n",
    "# Combine the average and standard deviation into the format \"avg ± std\"\n",
    "combined_values = avg_values.astype(str) + \" ± \" + std_values.astype(str)\n",
    "\n",
    "# Insert the LLM name at the beginning of the combined values\n",
    "combined_values = combined_values.tolist()\n",
    "combined_values.insert(0, LLM)\n",
    "\n",
    "# Create a DataFrame for the combined average ± std row\n",
    "result_df = pd.DataFrame([combined_values], columns=['Model'] + metrics_df.columns.tolist())\n",
    "\n",
    "# Add the ratio of invalid gen_response\n",
    "invalid_gen_res_ratio = df['gen_response'].isna().sum() /len(df) \n",
    "\n",
    "result_df['Failure Ratio'] = f\"{round(invalid_gen_res_ratio, 3)} ± 0.00\"  # No std for Failure Ratio\n",
    "\n",
    "result_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>response_time</th>\n",
       "      <th>Coh-UniEval</th>\n",
       "      <th>C Score</th>\n",
       "      <th>UE Score</th>\n",
       "      <th>Persona Distance</th>\n",
       "      <th>Failure Ratio</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Llama3-1-8B-Instruct</td>\n",
       "      <td>4.15 ± 0.53</td>\n",
       "      <td>0.82 ± 0.34</td>\n",
       "      <td>0.12 ± 0.66</td>\n",
       "      <td>0.23 ± 0.58</td>\n",
       "      <td>0.55 ± 0.22</td>\n",
       "      <td>0.098 ± 0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Mistral-7B-Instruct</td>\n",
       "      <td>3.11 ± 1.11</td>\n",
       "      <td>0.86 ± 0.28</td>\n",
       "      <td>0.36 ± 0.69</td>\n",
       "      <td>0.46 ± 0.72</td>\n",
       "      <td>0.62 ± 0.2</td>\n",
       "      <td>0.047 ± 0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Qwen2-7B-Instruct</td>\n",
       "      <td>1.97 ± 0.72</td>\n",
       "      <td>0.89 ± 0.24</td>\n",
       "      <td>0.23 ± 0.59</td>\n",
       "      <td>0.42 ± 0.73</td>\n",
       "      <td>0.61 ± 0.17</td>\n",
       "      <td>0.019 ± 0.00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  Model response_time  Coh-UniEval      C Score     UE Score  \\\n",
       "0  Llama3-1-8B-Instruct   4.15 ± 0.53  0.82 ± 0.34  0.12 ± 0.66  0.23 ± 0.58   \n",
       "1   Mistral-7B-Instruct   3.11 ± 1.11  0.86 ± 0.28  0.36 ± 0.69  0.46 ± 0.72   \n",
       "2     Qwen2-7B-Instruct   1.97 ± 0.72  0.89 ± 0.24  0.23 ± 0.59  0.42 ± 0.73   \n",
       "\n",
       "  Persona Distance Failure Ratio  \n",
       "0      0.55 ± 0.22  0.098 ± 0.00  \n",
       "1       0.62 ± 0.2  0.047 ± 0.00  \n",
       "2      0.61 ± 0.17  0.019 ± 0.00  "
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the existing Excel file and update or append the average row\n",
    "output_path = f'./Evaluations/{DATASET}{COT_}-results.xlsx'\n",
    "\n",
    "try:\n",
    "    # Load existing data\n",
    "    existing_df = pd.read_excel(output_path)\n",
    "    # Check if the model name already exists\n",
    "    if LLM in existing_df['Model'].values:\n",
    "        # Update the row with the same model name\n",
    "        existing_df.loc[existing_df['Model'] == LLM, :] = result_df.values\n",
    "    else:\n",
    "        # Append the new data\n",
    "        existing_df = pd.concat([existing_df, result_df], ignore_index=True)\n",
    "except FileNotFoundError:\n",
    "    # If the file does not exist, create a new DataFrame\n",
    "    existing_df = result_df\n",
    "\n",
    "# Save the updated DataFrame to an Excel file\n",
    "existing_df.to_excel(output_path, index=False)\n",
    "\n",
    "existing_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reviwing the Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>response_time</th>\n",
       "      <th>Coh-UniEval</th>\n",
       "      <th>C Score</th>\n",
       "      <th>UE Score</th>\n",
       "      <th>Persona Distance</th>\n",
       "      <th>Failure Ratio</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Llama3-1-8B-Instruct</td>\n",
       "      <td>4.15 ± 0.53</td>\n",
       "      <td>0.82 ± 0.34</td>\n",
       "      <td>0.12 ± 0.66</td>\n",
       "      <td>0.23 ± 0.58</td>\n",
       "      <td>0.55 ± 0.22</td>\n",
       "      <td>0.098 ± 0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Mistral-7B-Instruct</td>\n",
       "      <td>3.11 ± 1.11</td>\n",
       "      <td>0.86 ± 0.28</td>\n",
       "      <td>0.36 ± 0.69</td>\n",
       "      <td>0.46 ± 0.72</td>\n",
       "      <td>0.62 ± 0.2</td>\n",
       "      <td>0.047 ± 0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Qwen2-7B-Instruct</td>\n",
       "      <td>1.97 ± 0.72</td>\n",
       "      <td>0.89 ± 0.24</td>\n",
       "      <td>0.23 ± 0.59</td>\n",
       "      <td>0.42 ± 0.73</td>\n",
       "      <td>0.61 ± 0.17</td>\n",
       "      <td>0.019 ± 0.00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  Model response_time  Coh-UniEval      C Score     UE Score  \\\n",
       "0  Llama3-1-8B-Instruct   4.15 ± 0.53  0.82 ± 0.34  0.12 ± 0.66  0.23 ± 0.58   \n",
       "1   Mistral-7B-Instruct   3.11 ± 1.11  0.86 ± 0.28  0.36 ± 0.69  0.46 ± 0.72   \n",
       "2     Qwen2-7B-Instruct   1.97 ± 0.72  0.89 ± 0.24  0.23 ± 0.59  0.42 ± 0.73   \n",
       "\n",
       "  Persona Distance Failure Ratio  \n",
       "0      0.55 ± 0.22  0.098 ± 0.00  \n",
       "1       0.62 ± 0.2  0.047 ± 0.00  \n",
       "2      0.61 ± 0.17  0.019 ± 0.00  "
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# DATASET = \"FoCus\"  \n",
    "# # COT_ = \"-COT\"\n",
    "# COT_ =  \"\"\n",
    "\n",
    "response = pd.read_excel(f'./Evaluations/{DATASET}{COT_}-results.xlsx')\n",
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
