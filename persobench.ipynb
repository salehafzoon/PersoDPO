{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LLM Loading"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Auth Token Setting:\n",
    "\n",
    "- HugginigFace Token\n",
    "- OpenAI Token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline\n",
    "from dotenv import load_dotenv\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "import warnings\n",
    "import logging\n",
    "import torch\n",
    "import os\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "logging.getLogger('transformers').setLevel(logging.ERROR)\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "hf_token = os.getenv(\"HF_TOKEN\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "llms_info = {\n",
    "    \"Mistral-7B-Instruct\": {\n",
    "        \"remote_model_name\": \"mistralai/Mistral-7B-Instruct-v0.2\",\n",
    "        \"model_path\": \"./LLMs/Mistral-7B-Instruct\",\n",
    "        \"tokenizer_path\": \"./Tokenizers/Mistral-7B-Instruct\",\n",
    "        \"hf_token\": hf_token,\n",
    "        \"additional_config\": {\n",
    "            \"torch_dtype\": \"auto\",\n",
    "            \"device\": \"auto\"\n",
    "        }\n",
    "    },\n",
    "    \"Qwen2-7B-Instruct\": {\n",
    "        \"remote_model_name\": \"Qwen/Qwen2-7B-Instruct\",\n",
    "        \"model_path\": \"./LLMs/Qwen2-7B-Instruct\",\n",
    "        \"tokenizer_path\": \"./Tokenizers/Qwen2-7B-Instruct\",\n",
    "        \"hf_token\": hf_token,\n",
    "        \"additional_config\": {\n",
    "            \"torch_dtype\": \"auto\",\n",
    "            \"trust_remote_code\": True,\n",
    "            \"device\": \"auto\"\n",
    "        }\n",
    "    },\n",
    "    \"Qwen2-5B-Instruct\": {\n",
    "        \"remote_model_name\": \"Qwen/Qwen2-0.5B-Instruct\",\n",
    "        \"model_path\": \"./LLMs/Qwen2-5B-Instruct\",\n",
    "        \"tokenizer_path\": \"./Tokenizers/Qwen2-5B-Instruct\",\n",
    "        \"hf_token\": hf_token,\n",
    "        \"additional_config\": {\n",
    "            \"torch_dtype\": \"float32\",\n",
    "            \"trust_remote_code\": True,\n",
    "            \"device\": \"auto\"\n",
    "        }\n",
    "    }, \n",
    "    \"Llama3-1-8B-Instruct\": {\n",
    "    \"remote_model_name\": \"meta-llama/Meta-Llama-3.1-8B-Instruct\",\n",
    "    \"model_path\": \"./LLMs/Llama3-1-8B-Instruct\",\n",
    "    \"tokenizer_path\": \"./Tokenizers/Llama3-1-8B-Instruct\",\n",
    "    \"hf_token\": hf_token,\n",
    "    \"additional_config\": {\n",
    "        \"torch_dtype\": \"auto\",\n",
    "        \"device_map\": \"auto\",\n",
    "        \"rope_scaling\": {\n",
    "            \"type\": \"linear\",  # or \"dynamic\" — depending on your use case\n",
    "            \"factor\": 8.0\n",
    "        }\n",
    "    }\n",
    "},\n",
    "\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model(model_key):\n",
    "    model_info = llms_info[model_key]\n",
    "    config = model_info[\"additional_config\"]\n",
    "\n",
    "    # Check if the directories for the model and tokenizer exist\n",
    "    model_dir_exists = os.path.isdir(model_info[\"model_path\"])\n",
    "    tokenizer_dir_exists = os.path.isdir(model_info[\"tokenizer_path\"])\n",
    "\n",
    "    if model_dir_exists and tokenizer_dir_exists:\n",
    "        print(f\"{model_key} model and tokenizer are already present.\")\n",
    "    else:\n",
    "        print(f\"Downloading and saving model and tokenizer for {model_key}.\")\n",
    "        # Include the token in the download process if applicable\n",
    "        hf_token = model_info.get(\"hf_token\", None)\n",
    "        model = AutoModelForCausalLM.from_pretrained(\n",
    "            model_info[\"remote_model_name\"],\n",
    "            cache_dir=model_info[\"model_path\"],\n",
    "            torch_dtype=getattr(torch, config.get(\"torch_dtype\", \"auto\")) if config.get(\"torch_dtype\", \"auto\") != \"auto\" else None,\n",
    "            use_auth_token=hf_token\n",
    "        )\n",
    "        tokenizer = AutoTokenizer.from_pretrained(\n",
    "            model_info[\"remote_model_name\"],\n",
    "            cache_dir=model_info[\"tokenizer_path\"],\n",
    "            use_auth_token=hf_token\n",
    "        )\n",
    "        # Ensure directories are created during download\n",
    "        if not model_dir_exists:\n",
    "            os.makedirs(model_info[\"model_path\"], exist_ok=True)\n",
    "        if not tokenizer_dir_exists:\n",
    "            os.makedirs(model_info[\"tokenizer_path\"], exist_ok=True)\n",
    "        # Save them locally\n",
    "        model.save_pretrained(model_info[\"model_path\"])\n",
    "        tokenizer.save_pretrained(model_info[\"tokenizer_path\"])\n",
    "\n",
    "    # Load model and tokenizer from local storage\n",
    "    model = AutoModelForCausalLM.from_pretrained(model_info[\"model_path\"])\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_info[\"tokenizer_path\"])\n",
    "    return model, tokenizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_model(\"Qwen2-5B-Instruct\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Benchmarking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET = \"FoCus\"\n",
    "SET = \"train\"           #train,   valid       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "LLM = \"Qwen2-7B-Instruct\"                  # Mistral-7B-Instruct, Llama3-1-8B-Instruct, Qwen2-7B-Instruct\n",
    "\n",
    "COT_SETUP = False                               "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Response Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>personas</th>\n",
       "      <th>context</th>\n",
       "      <th>act_response</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Id like to visit a historic place.I want to vi...</td>\n",
       "      <td>User1: Wow, this is amazing! What is this?\\nUs...</td>\n",
       "      <td>User2: You can access Descent of the Ganges vi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>I hope to see some rock in Little Rock.I like ...</td>\n",
       "      <td>User1: I know this place, but I dont remember ...</td>\n",
       "      <td>User2: It was Sherman School.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I love cool lakes.I would like to visit the Hi...</td>\n",
       "      <td>User1: I know this place, but I dont remember ...</td>\n",
       "      <td>User2: It formed from being a tributary of the...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            personas  \\\n",
       "0  Id like to visit a historic place.I want to vi...   \n",
       "1  I hope to see some rock in Little Rock.I like ...   \n",
       "2  I love cool lakes.I would like to visit the Hi...   \n",
       "\n",
       "                                             context  \\\n",
       "0  User1: Wow, this is amazing! What is this?\\nUs...   \n",
       "1  User1: I know this place, but I dont remember ...   \n",
       "2  User1: I know this place, but I dont remember ...   \n",
       "\n",
       "                                        act_response  \n",
       "0  User2: You can access Descent of the Ganges vi...  \n",
       "1                      User2: It was Sherman School.  \n",
       "2  User2: It formed from being a tributary of the...  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.empty_cache()\n",
    "\n",
    "# Loading the prompt\n",
    "df = pd.read_csv(f'./Prompts/{DATASET}-{SET}.csv')\n",
    "\n",
    "df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to generate a prompt\n",
    "def create_benchmarking_prompt(personas, context, include_cot=False):\n",
    "\n",
    "    prompt = (\n",
    "        \"I will provide you with a conversation context and the personas of the participants, that can be annotated with speaker information.\\n\"\n",
    "        \"As a participant in this conversation, your task is to generate a personalized response, considering the conversation context and personas.\\n\\n\"\n",
    "        \"Participant Personas:\\n\"\n",
    "        f\"{personas}\\n\\n\"\n",
    "        \"Conversation Context:\\n\"\n",
    "        f\"{context}\\n\\n\"\n",
    "        \"Task Instruction:\\n\"\n",
    "        \"* Provide an unannotated response.\\n\"\n",
    "        \"* If only one persona is available, personalize the response accordingly.\\n\"\n",
    "        \"* If the conversation context is a single query, respond appropriately to the query.\\n\"\n",
    "    )\n",
    "    \n",
    "    if include_cot:\n",
    "        prompt += (\n",
    "            \"* Apply Chain of Thought reasoning to reflect on the alignment of your response with the personas.\\n\"\n",
    "        )\n",
    "\n",
    "    prompt += (\n",
    "        \"\\nOutput Format: only give a JSON of the following format:\\n\"\n",
    "        \"{\\n\"\n",
    "    )\n",
    "    \n",
    "    if include_cot:\n",
    "        prompt += (\n",
    "            '  \"reasoning\": \"briefly describe your personalization process (in 110 words or less).\"\\n'\n",
    "        )\n",
    "        \n",
    "    prompt += (\n",
    "        '  \"response\": \"provide the personalized natural language response here (in 110 words or less).\"\\n'\n",
    "        \"}\\n\"\n",
    "    )\n",
    "\n",
    "    return prompt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Qwen2-7B-Instruct model and tokenizer are already present.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cfe3c8bc125b43b98be38287ae7aa99e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "# Assuming load_model is defined and works as expected\n",
    "model, tokenizer = load_model(LLM)\n",
    "\n",
    "MAX_NEW_TOKEN = 220 if COT_SETUP else 110\n",
    "\n",
    "generation_params = {\n",
    "    \n",
    "    \"max_new_tokens\": MAX_NEW_TOKEN,      # Based on max response length + reasoning\n",
    "    \"temperature\": 0,                     # Based on FELM paper (Greedy Setup)\n",
    "    \"do_sample\": False\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I will provide you with a conversation context and the personas of the participants, that can be annotated with speaker information.\n",
      "As a participant in this conversation, your task is to generate a personalized response, considering the conversation context and personas.\n",
      "\n",
      "Participant Personas:\n",
      "I like university.I love old libraries.I would like to visit Iceland.I have never been to Reykjavik.I love books.\n",
      "\n",
      "Conversation Context:\n",
      "User1: I think Ive been there before but I dont remember the name of this place.\n",
      "User2: Youve never been here before, actually. This is The National University Library of Iceland, located in Reykjavik.\n",
      "User1: How old is it?\n",
      "User2: Youll love this old library, which dates back to 1818.\n",
      "User1: Does it have a lot of books?\n",
      "User2: Yes it does. Youll love the selection of books here, as this is the largest library in the country with millions of items in various collections.\n",
      "User1: What is all in their collection?\n",
      "User2: Their collection includes books, of course, but also newspapers, journals, and sound recordings.\n",
      "User1: Would I be able to visit?\n",
      "User2: Yes, you sure can. It is open for public access and you can acquire a library card for a small fee.\n",
      "User1: How large is the building?\n",
      "\n",
      "Task Instruction:\n",
      "* Provide an unannotated response.\n",
      "* If only one persona is available, personalize the response accordingly.\n",
      "* If the conversation context is a single query, respond appropriately to the query.\n",
      "\n",
      "Output Format: only give a JSON of the following format:\n",
      "{\n",
      "  \"response\": \"provide the personalized natural language response here (in 110 words or less).\"\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "first_row = df.iloc[120]\n",
    "personas = first_row['personas']\n",
    "context = first_row['context']\n",
    "\n",
    "# Example usage\n",
    "prompt = create_benchmarking_prompt(personas, context, include_cot=COT_SETUP)\n",
    "print(prompt)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"response\": \"Absolutely, I'd love to visit The National University Library of Iceland in Reykjavik! It's fascinating to know that it dates back to 1818, making it quite old. With millions of items across books, newspapers, journals, and sound recordings in its collection, it sounds like a treasure trove for book lovers like me. I'm excited about the possibility of getting a library card for a small fee and exploring the vast collection. And yes, I'm curious about the size of the building\n"
     ]
    }
   ],
   "source": [
    "# Initialize the text generation pipeline\n",
    "generator = pipeline('text-generation', model=model, tokenizer=tokenizer, device=device)\n",
    "\n",
    "# Generate a response\n",
    "output = generator(prompt, **generation_params)\n",
    "response = output[0]['generated_text'][len(prompt):]\n",
    "\n",
    "# Print the response\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating responses: 100%|██████████| 2000/2000 [2:12:56<00:00,  3.99s/it]  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>gen_response</th>\n",
       "      <th>response_time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>{\\n  \"response\": \"Descent of the Ganges is a r...</td>\n",
       "      <td>4.212004</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>{\\n  \"response\": \"Little Rock Central High Sch...</td>\n",
       "      <td>4.141517</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>{\\n  \"response\": \"The Manimahesh Lake is indee...</td>\n",
       "      <td>4.168975</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>{\\n  \"response\": \"Yes, the Camp Randall Stadiu...</td>\n",
       "      <td>4.286672</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>{\\n  \"response\": \"Urdaibai Bird Center in Spai...</td>\n",
       "      <td>4.294671</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>{\\n  \"response\": \"The climate of Mauna Kea is ...</td>\n",
       "      <td>4.060703</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>{\\n  \"response\": \"This is truly fascinating! I...</td>\n",
       "      <td>4.243618</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>{\\n  \"response\": \"This museum in Jerusalem, th...</td>\n",
       "      <td>4.169458</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>{\\n  \"response\": \"The area around Little Butte...</td>\n",
       "      <td>4.485641</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>{\\n  \"response\": \"This place, the Baltimore Co...</td>\n",
       "      <td>4.075697</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>{\\n  \"response\": \"The Stephen A. Schwarzman Bu...</td>\n",
       "      <td>3.897169</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>{\\n  \"response\": \"The first dam in Great Falls...</td>\n",
       "      <td>4.223865</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>{\\n  \"response\": \"When the Beijing Legation Qu...</td>\n",
       "      <td>4.322639</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>{\\n  \"response\": \"The SHI Stadium in New Jerse...</td>\n",
       "      <td>3.289952</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>{\\n  \"response\": \"As an architect, I'm fascina...</td>\n",
       "      <td>4.184443</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>{\\n  \"response\": \"The Cleveland Metroparks Zoo...</td>\n",
       "      <td>3.812050</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>{\\n  \"response\": \"Yes, the Old Melbourne Gaol ...</td>\n",
       "      <td>3.457716</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>{\\n  \"response\": \"Woodlawn Heights in the Bron...</td>\n",
       "      <td>4.150062</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>{\\n  \"response\": \"Yes, Ellisland Farm is indee...</td>\n",
       "      <td>3.921014</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>{\\n  \"response\": \"Henry Reed's name is the fir...</td>\n",
       "      <td>4.159723</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         gen_response  response_time\n",
       "0   {\\n  \"response\": \"Descent of the Ganges is a r...       4.212004\n",
       "1   {\\n  \"response\": \"Little Rock Central High Sch...       4.141517\n",
       "2   {\\n  \"response\": \"The Manimahesh Lake is indee...       4.168975\n",
       "3   {\\n  \"response\": \"Yes, the Camp Randall Stadiu...       4.286672\n",
       "4   {\\n  \"response\": \"Urdaibai Bird Center in Spai...       4.294671\n",
       "5   {\\n  \"response\": \"The climate of Mauna Kea is ...       4.060703\n",
       "6   {\\n  \"response\": \"This is truly fascinating! I...       4.243618\n",
       "7   {\\n  \"response\": \"This museum in Jerusalem, th...       4.169458\n",
       "8   {\\n  \"response\": \"The area around Little Butte...       4.485641\n",
       "9   {\\n  \"response\": \"This place, the Baltimore Co...       4.075697\n",
       "10  {\\n  \"response\": \"The Stephen A. Schwarzman Bu...       3.897169\n",
       "11  {\\n  \"response\": \"The first dam in Great Falls...       4.223865\n",
       "12  {\\n  \"response\": \"When the Beijing Legation Qu...       4.322639\n",
       "13  {\\n  \"response\": \"The SHI Stadium in New Jerse...       3.289952\n",
       "14  {\\n  \"response\": \"As an architect, I'm fascina...       4.184443\n",
       "15  {\\n  \"response\": \"The Cleveland Metroparks Zoo...       3.812050\n",
       "16  {\\n  \"response\": \"Yes, the Old Melbourne Gaol ...       3.457716\n",
       "17  {\\n  \"response\": \"Woodlawn Heights in the Bron...       4.150062\n",
       "18  {\\n  \"response\": \"Yes, Ellisland Farm is indee...       3.921014\n",
       "19  {\\n  \"response\": \"Henry Reed's name is the fir...       4.159723"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "# Iterate through the DataFrame and generate responses\n",
    "gen_responses = []\n",
    "response_times = []\n",
    "\n",
    "for index, row in tqdm(df.iterrows(), total=len(df), desc=\"Generating responses\"):\n",
    "    personas = row['personas']\n",
    "    context = row['context']\n",
    "    \n",
    "    # Create the prompt\n",
    "    prompt = create_benchmarking_prompt(personas, context, COT_SETUP)\n",
    "    \n",
    "    # Measure the start time\n",
    "    start_time = time.time()\n",
    "    \n",
    "    # Generate a response\n",
    "    output = generator(prompt, **generation_params)[0]['generated_text']\n",
    "    \n",
    "    # Measure the end time and calculate the duration\n",
    "    end_time = time.time()\n",
    "    response_time = end_time - start_time\n",
    "    \n",
    "    response = output[len(prompt):]\n",
    "\n",
    "    # Store the generated response and response time\n",
    "    gen_responses.append(response)\n",
    "    response_times.append(response_time)\n",
    "\n",
    "# Create a DataFrame with the responses and response times\n",
    "response_df = pd.DataFrame({\n",
    "    'gen_response': gen_responses,\n",
    "    'response_time': response_times\n",
    "})  \n",
    "\n",
    "COT_ = \"-COT\" if COT_SETUP else \"\" \n",
    "\n",
    "response_df.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Save the response DataFrame to a CSV and Excel file\n",
    "response_df.to_csv(f'./Raw Responses/{DATASET}/{LLM}-{SET}{COT_}.csv', index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Post Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape: (2000, 2)\n",
      "\n",
      "Missing Values:\n",
      " gen_response     0\n",
      "response_time    0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "ds = response_df\n",
    "print(\"Shape:\", ds.shape)\n",
    "\n",
    "print(\"\\nMissing Values:\\n\", ds.isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Missing Values:\n",
      " gen_response     1254\n",
      "response_time       0\n",
      "dtype: int64\n",
      "(2000, 2)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>gen_response</th>\n",
       "      <th>response_time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>None</td>\n",
       "      <td>4.212004</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>None</td>\n",
       "      <td>4.141517</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>None</td>\n",
       "      <td>4.168975</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>None</td>\n",
       "      <td>4.286672</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>None</td>\n",
       "      <td>4.294671</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1995</th>\n",
       "      <td>The Pelican Island National Wildlife Refuge is...</td>\n",
       "      <td>3.926378</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1996</th>\n",
       "      <td>The neighborhood today is quite different from...</td>\n",
       "      <td>4.008988</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1997</th>\n",
       "      <td>Yes, it's quite accessible! The National Archi...</td>\n",
       "      <td>3.260124</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1998</th>\n",
       "      <td>The Alhambra Creek is a beautiful stream locat...</td>\n",
       "      <td>4.147906</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1999</th>\n",
       "      <td>I've heard about the Brickyard 400, but I'm no...</td>\n",
       "      <td>3.289213</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2000 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           gen_response  response_time\n",
       "0                                                  None       4.212004\n",
       "1                                                  None       4.141517\n",
       "2                                                  None       4.168975\n",
       "3                                                  None       4.286672\n",
       "4                                                  None       4.294671\n",
       "...                                                 ...            ...\n",
       "1995  The Pelican Island National Wildlife Refuge is...       3.926378\n",
       "1996  The neighborhood today is quite different from...       4.008988\n",
       "1997  Yes, it's quite accessible! The National Archi...       3.260124\n",
       "1998  The Alhambra Creek is a beautiful stream locat...       4.147906\n",
       "1999  I've heard about the Brickyard 400, but I'm no...       3.289213\n",
       "\n",
       "[2000 rows x 2 columns]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "import json\n",
    "import pandas as pd\n",
    "\n",
    "# Define functions\n",
    "def find_first_valid_json(text):\n",
    "\n",
    "    \n",
    "    json_objects = re.findall(r'\\{.*?\\}', text, re.DOTALL)\n",
    "    for obj in json_objects:\n",
    "        try:\n",
    "            json_obj = json.loads(obj)\n",
    "            if \"response\" in json_obj:  # Only check for \"response\"\n",
    "                return json_obj\n",
    "        except json.JSONDecodeError:\n",
    "            continue\n",
    "    return None\n",
    "\n",
    "def get_response(text):\n",
    "    if text is not None:  # Check if text is not None\n",
    "        try:\n",
    "            return text['response']\n",
    "        except (ValueError, SyntaxError, KeyError):\n",
    "            return None\n",
    "    return None\n",
    "\n",
    "\n",
    "\n",
    "# Replace empty strings in 'gen_response' with None\n",
    "ds.loc[ds['gen_response'] == '', 'gen_response'] = None\n",
    "\n",
    "# Apply the find_first_valid_json function\n",
    "ds['gen_response'] = ds['gen_response'].apply(lambda x: find_first_valid_json(x))\n",
    "\n",
    "# Convert gen_response to None if it's not a valid string\n",
    "ds['gen_response'] = ds['gen_response'].apply(lambda x: None if pd.isna(x) or x == 'nan' or isinstance(x, float) else x)\n",
    "\n",
    "# Extract 'response' from the JSON objects\n",
    "ds['gen_response'] = ds['gen_response'].apply(lambda x: get_response(x))\n",
    "\n",
    "# Keep the 'response_time' column unchanged\n",
    "ds['response_time'] = ds['response_time']\n",
    "\n",
    "# Define the new column order\n",
    "new_column_order = ['gen_response', 'response_time']\n",
    "\n",
    "# Reorder the columns\n",
    "ds = ds[new_column_order]\n",
    "\n",
    "print(\"\\nMissing Values:\\n\", ds.isnull().sum())\n",
    "print(ds.shape)\n",
    "ds\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds.to_csv(f'Responses/{DATASET}/{LLM}-{SET}{COT_}.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
