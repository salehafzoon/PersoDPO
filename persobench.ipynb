{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LLM Loading"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Auth Token Setting:\n",
    "\n",
    "- HugginigFace Token\n",
    "- OpenAI Token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline\n",
    "from dotenv import load_dotenv\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "import warnings\n",
    "import logging\n",
    "import torch\n",
    "import os\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "logging.getLogger('transformers').setLevel(logging.ERROR)\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "hf_token = os.getenv(\"HF_TOKEN\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "llms_info = {\n",
    "    \"Mistral-7B-Instruct\": {\n",
    "        \"remote_model_name\": \"mistralai/Mistral-7B-Instruct-v0.2\",\n",
    "        \"model_path\": \"./LLMs/Mistral-7B-Instruct\",\n",
    "        \"tokenizer_path\": \"./Tokenizers/Mistral-7B-Instruct\",\n",
    "        \"hf_token\": hf_token,\n",
    "        \"additional_config\": {\n",
    "            \"torch_dtype\": \"auto\",\n",
    "            \"device\": \"auto\"\n",
    "        }\n",
    "    },\n",
    "    \"Qwen2-7B-Instruct\": {\n",
    "        \"remote_model_name\": \"Qwen/Qwen2-7B-Instruct\",\n",
    "        \"model_path\": \"./LLMs/Qwen2-7B-Instruct\",\n",
    "        \"tokenizer_path\": \"./Tokenizers/Qwen2-7B-Instruct\",\n",
    "        \"hf_token\": hf_token,\n",
    "        \"additional_config\": {\n",
    "            \"torch_dtype\": \"auto\",\n",
    "            \"trust_remote_code\": True,\n",
    "            \"device\": \"auto\"\n",
    "        }\n",
    "    },\n",
    "    \"Qwen2-5B-Instruct\": {\n",
    "        \"remote_model_name\": \"Qwen/Qwen2-0.5B-Instruct\",\n",
    "        \"model_path\": \"./LLMs/Qwen2-5B-Instruct\",\n",
    "        \"tokenizer_path\": \"./Tokenizers/Qwen2-5B-Instruct\",\n",
    "        \"hf_token\": hf_token,\n",
    "        \"additional_config\":{\n",
    "            \"trust_remote_code\": True,\n",
    "        }\n",
    "    }, \n",
    "    \"Llama3-1-8B-Instruct\": {\n",
    "    \"remote_model_name\": \"meta-llama/Meta-Llama-3.1-8B-Instruct\",\n",
    "    \"model_path\": \"./LLMs/Llama3-1-8B-Instruct\",\n",
    "    \"tokenizer_path\": \"./Tokenizers/Llama3-1-8B-Instruct\",\n",
    "    \"hf_token\": hf_token,\n",
    "    \"additional_config\": {\n",
    "        \"torch_dtype\": \"auto\",\n",
    "        \"device_map\": \"auto\",\n",
    "        \"rope_scaling\": {\n",
    "            \"type\": \"linear\",  # or \"dynamic\" — depending on your use case\n",
    "            \"factor\": 8.0\n",
    "        }\n",
    "    }\n",
    "},\n",
    "\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model(model_key):\n",
    "    model_info = llms_info[model_key]\n",
    "    config = model_info[\"additional_config\"]\n",
    "\n",
    "    # Check if the directories for the model and tokenizer exist\n",
    "    model_dir_exists = os.path.isdir(model_info[\"model_path\"])\n",
    "    tokenizer_dir_exists = os.path.isdir(model_info[\"tokenizer_path\"])\n",
    "\n",
    "    if model_dir_exists and tokenizer_dir_exists:\n",
    "        print(f\"{model_key} model and tokenizer are already present.\")\n",
    "    else:\n",
    "        print(f\"Downloading and saving model and tokenizer for {model_key}.\")\n",
    "        # Include the token in the download process if applicable\n",
    "        hf_token = model_info.get(\"hf_token\", None)\n",
    "        model = AutoModelForCausalLM.from_pretrained(\n",
    "            model_info[\"remote_model_name\"],\n",
    "            cache_dir=model_info[\"model_path\"],\n",
    "            torch_dtype=getattr(torch, config.get(\"torch_dtype\", \"auto\")) if config.get(\"torch_dtype\", \"auto\") != \"auto\" else None,\n",
    "            use_auth_token=hf_token\n",
    "        )\n",
    "        tokenizer = AutoTokenizer.from_pretrained(\n",
    "            model_info[\"remote_model_name\"],\n",
    "            cache_dir=model_info[\"tokenizer_path\"],\n",
    "            use_auth_token=hf_token\n",
    "        )\n",
    "        # Ensure directories are created during download\n",
    "        if not model_dir_exists:\n",
    "            os.makedirs(model_info[\"model_path\"], exist_ok=True)\n",
    "        if not tokenizer_dir_exists:\n",
    "            os.makedirs(model_info[\"tokenizer_path\"], exist_ok=True)\n",
    "        # Save them locally\n",
    "        model.save_pretrained(model_info[\"model_path\"])\n",
    "        tokenizer.save_pretrained(model_info[\"tokenizer_path\"])\n",
    "\n",
    "    # Load model and tokenizer from local storage\n",
    "    model = AutoModelForCausalLM.from_pretrained(model_info[\"model_path\"])\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_info[\"tokenizer_path\"])\n",
    "    return model, tokenizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load_model(\"Qwen2-5B-Instruct\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Benchmarking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET = \"FoCus\"\n",
    "SET = \"train\"           #train,   valid       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "LLM = \"Qwen2-7B-Instruct\"                  # Mistral-7B-Instruct, Llama3-1-8B-Instruct, Qwen2-7B-Instruct, Qwen2-5B-Instruct\n",
    "\n",
    "COT_SETUP = False                               "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Response Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>personas</th>\n",
       "      <th>context</th>\n",
       "      <th>act_response</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Id like to visit a historic place.I want to vi...</td>\n",
       "      <td>User1: Wow, this is amazing! What is this?\\nUs...</td>\n",
       "      <td>User2: You can access Descent of the Ganges vi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>I hope to see some rock in Little Rock.I like ...</td>\n",
       "      <td>User1: I know this place, but I dont remember ...</td>\n",
       "      <td>User2: It was Sherman School.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I love cool lakes.I would like to visit the Hi...</td>\n",
       "      <td>User1: I know this place, but I dont remember ...</td>\n",
       "      <td>User2: It formed from being a tributary of the...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            personas  \\\n",
       "0  Id like to visit a historic place.I want to vi...   \n",
       "1  I hope to see some rock in Little Rock.I like ...   \n",
       "2  I love cool lakes.I would like to visit the Hi...   \n",
       "\n",
       "                                             context  \\\n",
       "0  User1: Wow, this is amazing! What is this?\\nUs...   \n",
       "1  User1: I know this place, but I dont remember ...   \n",
       "2  User1: I know this place, but I dont remember ...   \n",
       "\n",
       "                                        act_response  \n",
       "0  User2: You can access Descent of the Ganges vi...  \n",
       "1                      User2: It was Sherman School.  \n",
       "2  User2: It formed from being a tributary of the...  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.empty_cache()\n",
    "\n",
    "# Loading the prompt\n",
    "df = pd.read_csv(f'./Prompts/{DATASET}-{SET}.csv')\n",
    "\n",
    "df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to generate a prompt\n",
    "def create_benchmarking_prompt(personas, context, include_cot=False):\n",
    "\n",
    "    prompt = (\n",
    "        \"I will provide you with a conversation context and the personas of the participants, that can be annotated with speaker information.\\n\"\n",
    "        \"As a participant in this conversation, your task is to generate a personalized response, considering the conversation context and personas.\\n\\n\"\n",
    "        \"Participant Personas:\\n\"\n",
    "        f\"{personas}\\n\\n\"\n",
    "        \"Conversation Context:\\n\"\n",
    "        f\"{context}\\n\\n\"\n",
    "        \"Task Instruction:\\n\"\n",
    "        \"* Provide an unannotated response.\\n\"\n",
    "        \"* If only one persona is available, personalize the response accordingly.\\n\"\n",
    "        \"* If the conversation context is a single query, respond appropriately to the query.\\n\"\n",
    "    )\n",
    "    \n",
    "    if include_cot:\n",
    "        prompt += (\n",
    "            \"* Apply Chain of Thought reasoning to reflect on the alignment of your response with the personas.\\n\"\n",
    "        )\n",
    "\n",
    "    prompt += (\n",
    "        \"\\nOutput Format: only give a JSON of the following format:\\n\"\n",
    "        \"{\\n\"\n",
    "    )\n",
    "    \n",
    "    if include_cot:\n",
    "        prompt += (\n",
    "            '  \"reasoning\": \"briefly describe your personalization process (in 110 words or less).\"\\n'\n",
    "        )\n",
    "        \n",
    "    prompt += (\n",
    "        '  \"response\": \"provide the personalized natural language response here (in 110 words or less).\"\\n'\n",
    "        \"}\\n\"\n",
    "    )\n",
    "\n",
    "    return prompt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Qwen2-7B-Instruct model and tokenizer are already present.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f945f69f7435417095010ee6132db603",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Assuming load_model is defined and works as expected\n",
    "model, tokenizer = load_model(LLM)\n",
    "\n",
    "MAX_NEW_TOKEN = 220 if COT_SETUP else 110\n",
    "\n",
    "generation_params = {\n",
    "    \n",
    "    \"max_new_tokens\": MAX_NEW_TOKEN,      # Based on max response length + reasoning\n",
    "    \"temperature\": 0,                     # Based on FELM paper (Greedy Setup)\n",
    "    \"do_sample\": False\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I will provide you with a conversation context and the personas of the participants, that can be annotated with speaker information.\n",
      "As a participant in this conversation, your task is to generate a personalized response, considering the conversation context and personas.\n",
      "\n",
      "Participant Personas:\n",
      "I like university.I love old libraries.I would like to visit Iceland.I have never been to Reykjavik.I love books.\n",
      "\n",
      "Conversation Context:\n",
      "User1: I think Ive been there before but I dont remember the name of this place.\n",
      "User2: Youve never been here before, actually. This is The National University Library of Iceland, located in Reykjavik.\n",
      "User1: How old is it?\n",
      "User2: Youll love this old library, which dates back to 1818.\n",
      "User1: Does it have a lot of books?\n",
      "User2: Yes it does. Youll love the selection of books here, as this is the largest library in the country with millions of items in various collections.\n",
      "User1: What is all in their collection?\n",
      "User2: Their collection includes books, of course, but also newspapers, journals, and sound recordings.\n",
      "User1: Would I be able to visit?\n",
      "User2: Yes, you sure can. It is open for public access and you can acquire a library card for a small fee.\n",
      "User1: How large is the building?\n",
      "\n",
      "Task Instruction:\n",
      "* Provide an unannotated response.\n",
      "* If only one persona is available, personalize the response accordingly.\n",
      "* If the conversation context is a single query, respond appropriately to the query.\n",
      "\n",
      "Output Format: only give a JSON of the following format:\n",
      "{\n",
      "  \"response\": \"provide the personalized natural language response here (in 110 words or less).\"\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "first_row = df.iloc[120]\n",
    "personas = first_row['personas']\n",
    "context = first_row['context']\n",
    "\n",
    "# Example usage\n",
    "prompt = create_benchmarking_prompt(personas, context, include_cot=COT_SETUP)\n",
    "print(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"response\": \"Absolutely, I'd love to visit The National University Library of Iceland in Reykjavik! It's fascinating to know that it dates back to 1818, making it quite old. With millions of items across books, newspapers, journals, and sound recordings in its collection, it sounds like a treasure trove for book lovers like me. I'm excited about the possibility of getting a library card for a small fee and exploring the vast collection. And yes, I'm curious about the size of the building\n"
     ]
    }
   ],
   "source": [
    "# Initialize the text generation pipeline\n",
    "generator = pipeline('text-generation', model=model, tokenizer=tokenizer, device=device)\n",
    "\n",
    "# Generate a response\n",
    "output = generator(prompt, **generation_params)\n",
    "response = output[0]['generated_text'][len(prompt):]\n",
    "\n",
    "# Print the response\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "# Iterate through the DataFrame and generate responses\n",
    "gen_responses = []\n",
    "response_times = []\n",
    "\n",
    "for index, row in tqdm(df.iterrows(), total=len(df), desc=\"Generating responses\"):\n",
    "    personas = row['personas']\n",
    "    context = row['context']\n",
    "    \n",
    "    # Create the prompt\n",
    "    prompt = create_benchmarking_prompt(personas, context, COT_SETUP)\n",
    "    \n",
    "    # Measure the start time\n",
    "    start_time = time.time()\n",
    "    \n",
    "    # Generate a response\n",
    "    output = generator(prompt, **generation_params)[0]['generated_text']\n",
    "    \n",
    "    # Measure the end time and calculate the duration\n",
    "    end_time = time.time()\n",
    "    response_time = end_time - start_time\n",
    "    \n",
    "    response = output[len(prompt):]\n",
    "\n",
    "    # Store the generated response and response time\n",
    "    gen_responses.append(response)\n",
    "    response_times.append(response_time)\n",
    "\n",
    "# Create a DataFrame with the responses and response times\n",
    "response_df = pd.DataFrame({\n",
    "    'gen_response': gen_responses,\n",
    "    'response_time': response_times\n",
    "})  \n",
    "\n",
    "COT_ = \"-COT\" if COT_SETUP else \"\" \n",
    "\n",
    "response_df.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Missing Values:\n",
      " gen_response     0\n",
      "response_time    0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nMissing Values:\\n\", response_df.isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Save the response DataFrame to a CSV and Excel file\n",
    "response_df.to_csv(f'./Raw Responses/{DATASET}/{LLM}-train{COT_}.csv', index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Post Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape: (4000, 2)\n",
      "\n",
      "Missing Values:\n",
      " gen_response     0\n",
      "response_time    0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "ds = response_df\n",
    "print(\"Shape:\", ds.shape)\n",
    "\n",
    "print(\"\\nMissing Values:\\n\", ds.isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Missing Values:\n",
      " gen_response     1536\n",
      "response_time       0\n",
      "dtype: int64\n",
      "(4000, 2)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>gen_response</th>\n",
       "      <th>response_time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Hello! Welcome to Descent of the Ganges. It's ...</td>\n",
       "      <td>0.790489</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>The name of the place is Little Rock Central H...</td>\n",
       "      <td>0.744353</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>None</td>\n",
       "      <td>0.895105</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Camp Randall Stadium is an outdoor stadium loc...</td>\n",
       "      <td>0.747605</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>None</td>\n",
       "      <td>0.898532</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3995</th>\n",
       "      <td>None</td>\n",
       "      <td>0.895923</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3996</th>\n",
       "      <td>Kõpu Lighthouse is a beautiful historical land...</td>\n",
       "      <td>0.676165</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3997</th>\n",
       "      <td>St. Matthews Lutheran Church is a historic chu...</td>\n",
       "      <td>0.828035</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3998</th>\n",
       "      <td>The bell has been restored to its original pos...</td>\n",
       "      <td>0.339198</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3999</th>\n",
       "      <td>The castle is located in the northwest of the ...</td>\n",
       "      <td>0.879601</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4000 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           gen_response  response_time\n",
       "0     Hello! Welcome to Descent of the Ganges. It's ...       0.790489\n",
       "1     The name of the place is Little Rock Central H...       0.744353\n",
       "2                                                  None       0.895105\n",
       "3     Camp Randall Stadium is an outdoor stadium loc...       0.747605\n",
       "4                                                  None       0.898532\n",
       "...                                                 ...            ...\n",
       "3995                                               None       0.895923\n",
       "3996  Kõpu Lighthouse is a beautiful historical land...       0.676165\n",
       "3997  St. Matthews Lutheran Church is a historic chu...       0.828035\n",
       "3998  The bell has been restored to its original pos...       0.339198\n",
       "3999  The castle is located in the northwest of the ...       0.879601\n",
       "\n",
       "[4000 rows x 2 columns]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "import json\n",
    "import pandas as pd\n",
    "\n",
    "# Define functions\n",
    "def find_first_valid_json(text):\n",
    "    if not isinstance(text, str) or text.strip() == \"\":\n",
    "        return None  # skip if not a string or empty string\n",
    "    \n",
    "    json_objects = re.findall(r'\\{.*?\\}', text, re.DOTALL)\n",
    "    for obj in json_objects:\n",
    "        try:\n",
    "            json_obj = json.loads(obj)\n",
    "            if \"response\" in json_obj:  # Only check for \"response\"\n",
    "                return json_obj\n",
    "        except json.JSONDecodeError:\n",
    "            continue\n",
    "    return None\n",
    "\n",
    "def get_response(text):\n",
    "    if text is not None:  # Check if text is not None\n",
    "        try:\n",
    "            return text['response']\n",
    "        except (ValueError, SyntaxError, KeyError):\n",
    "            return None\n",
    "    return None\n",
    "\n",
    "\n",
    "# Replace empty strings in 'gen_response' with None\n",
    "ds.loc[ds['gen_response'] == '', 'gen_response'] = None\n",
    "\n",
    "# Apply the find_first_valid_json function\n",
    "ds['gen_response'] = ds['gen_response'].apply(lambda x: find_first_valid_json(x))\n",
    "\n",
    "# Convert gen_response to None if it's not a valid string\n",
    "ds['gen_response'] = ds['gen_response'].apply(lambda x: None if pd.isna(x) or x == 'nan' or isinstance(x, float) else x)\n",
    "\n",
    "# Extract 'response' from the JSON objects\n",
    "ds['gen_response'] = ds['gen_response'].apply(lambda x: get_response(x))\n",
    "\n",
    "# Keep the 'response_time' column unchanged\n",
    "ds['response_time'] = ds['response_time']\n",
    "\n",
    "# Define the new column order\n",
    "new_column_order = ['gen_response', 'response_time']\n",
    "\n",
    "# Reorder the columns\n",
    "ds = ds[new_column_order]\n",
    "\n",
    "print(\"\\nMissing Values:\\n\", ds.isnull().sum())\n",
    "print(ds.shape)\n",
    "ds\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds.to_csv(f'Responses/{DATASET}/{LLM}-train{COT_}.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
