{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LLM Loading"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Auth Token Setting:\n",
    "\n",
    "- HugginigFace Token\n",
    "- OpenAI Token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline\n",
    "from dotenv import load_dotenv\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "import warnings\n",
    "import logging\n",
    "import torch\n",
    "import os\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "logging.getLogger('transformers').setLevel(logging.ERROR)\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "hf_token = os.getenv(\"HF_TOKEN\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "llms_info = {\n",
    "    \"Mistral-7B-Instruct\": {\n",
    "        \"remote_model_name\": \"mistralai/Mistral-7B-Instruct-v0.3\",\n",
    "        \"model_path\": \"./LLMs/Mistral-7B-Instruct\",\n",
    "        \"tokenizer_path\": \"./Tokenizers/Mistral-7B-Instruct\",\n",
    "        \"hf_token\": hf_token,\n",
    "        \"additional_config\": {\n",
    "            \"torch_dtype\": \"auto\",\n",
    "            \"device\": \"auto\"\n",
    "        }\n",
    "    },\n",
    "    \"Qwen2-7B-Instruct\": {\n",
    "        \"remote_model_name\": \"Qwen/Qwen2-7B-Instruct\",\n",
    "        \"model_path\": \"./LLMs/Qwen2-7B-Instruct\",\n",
    "        \"tokenizer_path\": \"./Tokenizers/Qwen2-7B-Instruct\",\n",
    "        \"hf_token\": hf_token,\n",
    "        \"additional_config\": {\n",
    "            \"torch_dtype\": \"auto\",\n",
    "            \"trust_remote_code\": True,\n",
    "            \"device\": \"auto\"\n",
    "        }\n",
    "    },\n",
    "    \"Qwen2-5B-Instruct\": {\n",
    "        \"remote_model_name\": \"Qwen/Qwen2-0.5B-Instruct\",\n",
    "        \"model_path\": \"./LLMs/Qwen2-5B-Instruct\",\n",
    "        \"tokenizer_path\": \"./Tokenizers/Qwen2-5B-Instruct\",\n",
    "        \"hf_token\": hf_token,\n",
    "        \"additional_config\":{\n",
    "            \"trust_remote_code\": True,\n",
    "        }\n",
    "    }, \n",
    "    \"Llama3-1-8B-Instruct\": {\n",
    "    \"remote_model_name\": \"meta-llama/Llama-3.1-8B-Instruct\",\n",
    "    \"model_path\": \"./LLMs/Llama3-1-8B-Instruct\",\n",
    "    \"tokenizer_path\": \"./Tokenizers/Llama3-1-8B-Instruct\",\n",
    "    \"hf_token\": hf_token,\n",
    "    \"additional_config\": {\n",
    "        \"torch_dtype\": \"auto\",\n",
    "        \"device_map\": \"auto\",\n",
    "        \"rope_scaling\": {\n",
    "            \"type\": \"linear\",  # or \"dynamic\" — depending on your use case\n",
    "            \"factor\": 8.0\n",
    "        }\n",
    "    }\n",
    "},\n",
    "\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model(model_key):\n",
    "    model_info = llms_info[model_key]\n",
    "    config = model_info[\"additional_config\"]\n",
    "\n",
    "    # Check if the directories for the model and tokenizer exist\n",
    "    model_dir_exists = os.path.isdir(model_info[\"model_path\"])\n",
    "    tokenizer_dir_exists = os.path.isdir(model_info[\"tokenizer_path\"])\n",
    "\n",
    "    if model_dir_exists and tokenizer_dir_exists:\n",
    "        print(f\"{model_key} model and tokenizer are already present.\")\n",
    "    else:\n",
    "        print(f\"Downloading and saving model and tokenizer for {model_key}.\")\n",
    "        # Include the token in the download process if applicable\n",
    "        hf_token = model_info.get(\"hf_token\", None)\n",
    "        model = AutoModelForCausalLM.from_pretrained(\n",
    "            model_info[\"remote_model_name\"],\n",
    "            cache_dir=model_info[\"model_path\"],\n",
    "            torch_dtype=getattr(torch, config.get(\"torch_dtype\", \"auto\")) if config.get(\"torch_dtype\", \"auto\") != \"auto\" else None,\n",
    "            use_auth_token=hf_token\n",
    "        )\n",
    "        tokenizer = AutoTokenizer.from_pretrained(\n",
    "            model_info[\"remote_model_name\"],\n",
    "            cache_dir=model_info[\"tokenizer_path\"],\n",
    "            use_auth_token=hf_token\n",
    "        )\n",
    "        # Ensure directories are created during download\n",
    "        if not model_dir_exists:\n",
    "            os.makedirs(model_info[\"model_path\"], exist_ok=True)\n",
    "        if not tokenizer_dir_exists:\n",
    "            os.makedirs(model_info[\"tokenizer_path\"], exist_ok=True)\n",
    "        # Save them locally\n",
    "        model.save_pretrained(model_info[\"model_path\"])\n",
    "        tokenizer.save_pretrained(model_info[\"tokenizer_path\"])\n",
    "\n",
    "    # Load model and tokenizer from local storage\n",
    "    model = AutoModelForCausalLM.from_pretrained(model_info[\"model_path\"])\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_info[\"tokenizer_path\"])\n",
    "    return model, tokenizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load_model(\"Mistral-7B-Instruct\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Benchmarking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET = \"FoCus\"\n",
    "SET = \"train\"           #train,   valid       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "LLM = \"Qwen2-5B-Instruct\"                  # Mistral-7B-Instruct, Llama3-1-8B-Instruct, Qwen2-7B-Instruct, Qwen2-5B-Instruct\n",
    "\n",
    "COT_SETUP = False                               "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Response Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1500, 3)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>personas</th>\n",
       "      <th>context</th>\n",
       "      <th>act_response</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>I want to learn more about the Royal Navy.I ha...</td>\n",
       "      <td>User1: I know this place, but I dont remember ...</td>\n",
       "      <td>User2: The shit was controversial he scrapped ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>I collect lighthouses.Ive never been to to Eng...</td>\n",
       "      <td>User1: Wow, this is amazing! What is this?\\nUs...</td>\n",
       "      <td>User2: Yes! An episode from ChuckleVision and ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I am interested in region.I would like to visi...</td>\n",
       "      <td>User1: Where is this place?\\nUser2: This is Ac...</td>\n",
       "      <td>User2: Waterways are essential to the commerci...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            personas  \\\n",
       "0  I want to learn more about the Royal Navy.I ha...   \n",
       "1  I collect lighthouses.Ive never been to to Eng...   \n",
       "2  I am interested in region.I would like to visi...   \n",
       "\n",
       "                                             context  \\\n",
       "0  User1: I know this place, but I dont remember ...   \n",
       "1  User1: Wow, this is amazing! What is this?\\nUs...   \n",
       "2  User1: Where is this place?\\nUser2: This is Ac...   \n",
       "\n",
       "                                        act_response  \n",
       "0  User2: The shit was controversial he scrapped ...  \n",
       "1  User2: Yes! An episode from ChuckleVision and ...  \n",
       "2  User2: Waterways are essential to the commerci...  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.empty_cache()\n",
    "\n",
    "# Loading the prompt\n",
    "df = pd.read_csv(f'./Prompts/{DATASET}-{SET}.csv')\n",
    "\n",
    "\n",
    "print(df.shape)\n",
    "df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to generate a prompt\n",
    "def create_benchmarking_prompt(personas, context, include_cot=False):\n",
    "\n",
    "    prompt = (\n",
    "        \"I will provide you with a conversation context and the personas of the participants, that can be annotated with speaker information.\\n\"\n",
    "        \"As a participant in this conversation, your task is to generate a personalized response, considering the conversation context and personas.\\n\\n\"\n",
    "        \"Participant Personas:\\n\"\n",
    "        f\"{personas}\\n\\n\"\n",
    "        \"Conversation Context:\\n\"\n",
    "        f\"{context}\\n\\n\"\n",
    "        \"Task Instruction:\\n\"\n",
    "        \"* Provide an unannotated response.\\n\"\n",
    "        \"* If only one persona is available, personalize the response accordingly.\\n\"\n",
    "        \"* If the conversation context is a single query, respond appropriately to the query.\\n\"\n",
    "    )\n",
    "    \n",
    "    if include_cot:\n",
    "        prompt += (\n",
    "            \"* Apply Chain of Thought reasoning to reflect on the alignment of your response with the personas.\\n\"\n",
    "        )\n",
    "\n",
    "    prompt += (\n",
    "        \"\\nOutput Format: only give a JSON of the following format:\\n\"\n",
    "        \"{\\n\"\n",
    "    )\n",
    "    \n",
    "    if include_cot:\n",
    "        prompt += (\n",
    "            '  \"reasoning\": \"briefly describe your personalization process (in 110 words or less).\"\\n'\n",
    "        )\n",
    "        \n",
    "    prompt += (\n",
    "        '  \"response\": \"provide the personalized natural language response here (in 110 words or less).\"\\n'\n",
    "        \"}\\n\"\n",
    "    )\n",
    "\n",
    "    return prompt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Qwen2-5B-Instruct model and tokenizer are already present.\n"
     ]
    }
   ],
   "source": [
    "# Assuming load_model is defined and works as expected\n",
    "model, tokenizer = load_model(LLM)\n",
    "\n",
    "MAX_NEW_TOKEN = 220 if COT_SETUP else 110\n",
    "\n",
    "generation_params = {\n",
    "    \n",
    "    \"max_new_tokens\": MAX_NEW_TOKEN,      # Based on max response length + reasoning\n",
    "    \"temperature\": 0,                     # Based on FELM paper (Greedy Setup)\n",
    "    \"do_sample\": False\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I will provide you with a conversation context and the personas of the participants, that can be annotated with speaker information.\n",
      "As a participant in this conversation, your task is to generate a personalized response, considering the conversation context and personas.\n",
      "\n",
      "Participant Personas:\n",
      "I have the fantasy about mountain.I like hill fort.I have heard about Celtic centres.I would like to go to Germany.I like rivers.\n",
      "\n",
      "Conversation Context:\n",
      "User1: Wow, this is amazing! What is this?\n",
      "User2: This will be your favorite place. The Heuneburg is a prehistoric hillfort in Germany, the country you want to visit.\n",
      "User1: Nice! Where is exactly this place located?\n",
      "User2: Its located in Hundersingen near Herbertingen, between Ulm and Sigmaringen, Baden-Württemberg, in the south of Germany.\n",
      "User1: Tell me more about the surroundings of the site.\n",
      "User2: Its on the side of the river Danube. Also, it is near to the modern borders with Switzerland and Austria.\n",
      "User1: What is the significance of the site?\n",
      "User2: You have heard about Celtic centres; the fort is considered to be one of the most important early Celtic centres in Central Europe.\n",
      "User1: When the fort was abandoned?\n",
      "User2: The fort was abandoned in the 5th century BC.\n",
      "User1: What is the height of the fort above the river?\n",
      "\n",
      "Task Instruction:\n",
      "* Provide an unannotated response.\n",
      "* If only one persona is available, personalize the response accordingly.\n",
      "* If the conversation context is a single query, respond appropriately to the query.\n",
      "\n",
      "Output Format: only give a JSON of the following format:\n",
      "{\n",
      "  \"response\": \"provide the personalized natural language response here (in 110 words or less).\"\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "first_row = df.iloc[120]\n",
    "personas = first_row['personas']\n",
    "context = first_row['context']\n",
    "\n",
    "# Example usage\n",
    "prompt = create_benchmarking_prompt(personas, context, include_cot=COT_SETUP)\n",
    "print(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: Please include all necessary elements from the given personas in the response.\n",
      "\n",
      "{\"response\": \"The Heuneburg is a prehistoric hillfort in Germany, the country you want to visit. It's located in Hundersingen near Herbertingen, between Ulm and Sigmaringen, Baden-Württemberg, in the south of Germany. The Heuneburg was abandoned in the 5th century BC.\"}\n"
     ]
    }
   ],
   "source": [
    "# Initialize the text generation pipeline\n",
    "generator = pipeline('text-generation', model=model, tokenizer=tokenizer, device=device)\n",
    "\n",
    "# Generate a response\n",
    "output = generator(prompt, **generation_params)\n",
    "response = output[0]['generated_text'][len(prompt):]\n",
    "\n",
    "# Print the response\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating responses: 100%|██████████| 1500/1500 [19:27<00:00,  1.28it/s]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>gen_response</th>\n",
       "      <th>response_time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Note: You may need to break down the response ...</td>\n",
       "      <td>0.792666</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Note: You may need to break down the response ...</td>\n",
       "      <td>0.813223</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Note: You may need to break down the response ...</td>\n",
       "      <td>0.892982</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Note: You may need to add additional details o...</td>\n",
       "      <td>0.898334</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Note: You may not change any of the placeholde...</td>\n",
       "      <td>0.895691</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Note: The response should be personal and tail...</td>\n",
       "      <td>0.784738</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Note: The response should not contain any pers...</td>\n",
       "      <td>0.886195</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Note: You may need to add additional details i...</td>\n",
       "      <td>0.892098</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Note: The response should not contain any pers...</td>\n",
       "      <td>0.901373</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Note: You may need to add additional details i...</td>\n",
       "      <td>0.836608</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Note: You may not use any external resources e...</td>\n",
       "      <td>0.714249</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Note: The response should not contain any pers...</td>\n",
       "      <td>0.691741</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Note: The response should not contain any pers...</td>\n",
       "      <td>0.602580</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Note: The response should be personal and tail...</td>\n",
       "      <td>0.707536</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>Note: The response should not contain any pers...</td>\n",
       "      <td>0.747988</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>Note: You may need to break down the response ...</td>\n",
       "      <td>0.493099</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>Note: The response should not contain any pers...</td>\n",
       "      <td>0.871162</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>Note: The response should be personal and tail...</td>\n",
       "      <td>0.798430</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>Note: Please include all necessary elements fr...</td>\n",
       "      <td>0.888124</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>Note: You may need to add additional details i...</td>\n",
       "      <td>0.784153</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         gen_response  response_time\n",
       "0   Note: You may need to break down the response ...       0.792666\n",
       "1   Note: You may need to break down the response ...       0.813223\n",
       "2   Note: You may need to break down the response ...       0.892982\n",
       "3   Note: You may need to add additional details o...       0.898334\n",
       "4   Note: You may not change any of the placeholde...       0.895691\n",
       "5   Note: The response should be personal and tail...       0.784738\n",
       "6   Note: The response should not contain any pers...       0.886195\n",
       "7   Note: You may need to add additional details i...       0.892098\n",
       "8   Note: The response should not contain any pers...       0.901373\n",
       "9   Note: You may need to add additional details i...       0.836608\n",
       "10  Note: You may not use any external resources e...       0.714249\n",
       "11  Note: The response should not contain any pers...       0.691741\n",
       "12  Note: The response should not contain any pers...       0.602580\n",
       "13  Note: The response should be personal and tail...       0.707536\n",
       "14  Note: The response should not contain any pers...       0.747988\n",
       "15  Note: You may need to break down the response ...       0.493099\n",
       "16  Note: The response should not contain any pers...       0.871162\n",
       "17  Note: The response should be personal and tail...       0.798430\n",
       "18  Note: Please include all necessary elements fr...       0.888124\n",
       "19  Note: You may need to add additional details i...       0.784153"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "# Iterate through the DataFrame and generate responses\n",
    "gen_responses = []\n",
    "response_times = []\n",
    "\n",
    "for index, row in tqdm(df.iterrows(), total=len(df), desc=\"Generating responses\"):\n",
    "    personas = row['personas']\n",
    "    context = row['context']\n",
    "    \n",
    "    # Create the prompt\n",
    "    prompt = create_benchmarking_prompt(personas, context, COT_SETUP)\n",
    "    \n",
    "    # Measure the start time\n",
    "    start_time = time.time()\n",
    "    \n",
    "    # Generate a response\n",
    "    output = generator(prompt, **generation_params)[0]['generated_text']\n",
    "    \n",
    "    # Measure the end time and calculate the duration\n",
    "    end_time = time.time()\n",
    "    response_time = end_time - start_time\n",
    "    \n",
    "    response = output[len(prompt):]\n",
    "\n",
    "    # Store the generated response and response time\n",
    "    gen_responses.append(response)\n",
    "    response_times.append(response_time)\n",
    "\n",
    "# Create a DataFrame with the responses and response times\n",
    "response_df = pd.DataFrame({\n",
    "    'gen_response': gen_responses,\n",
    "    'response_time': response_times\n",
    "})  \n",
    "\n",
    "COT_ = \"-COT\" if COT_SETUP else \"\"\n",
    "\n",
    "response_df.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Missing Values:\n",
      " gen_response     0\n",
      "response_time    0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nMissing Values:\\n\", response_df.isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Save the response DataFrame to a CSV and Excel file\n",
    "response_df.to_csv(f'./Raw Responses/{DATASET}/{LLM}-train{COT_}.csv', index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Post Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape: (1500, 2)\n",
      "\n",
      "Missing Values:\n",
      " gen_response     0\n",
      "response_time    0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "ds = response_df\n",
    "print(\"Shape:\", ds.shape)\n",
    "\n",
    "print(\"\\nMissing Values:\\n\", ds.isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Missing Values:\n",
      " gen_response     592\n",
      "response_time      0\n",
      "dtype: int64\n",
      "(1500, 2)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>gen_response</th>\n",
       "      <th>response_time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>The HMS Plymouth is a frigate belonging to the...</td>\n",
       "      <td>0.792666</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>The South Foreland Lighthouse is located in So...</td>\n",
       "      <td>0.813223</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>None</td>\n",
       "      <td>0.892982</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>None</td>\n",
       "      <td>0.898334</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>None</td>\n",
       "      <td>0.895691</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1495</th>\n",
       "      <td>None</td>\n",
       "      <td>0.881157</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1496</th>\n",
       "      <td>None</td>\n",
       "      <td>0.904275</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1497</th>\n",
       "      <td>The Vulci were a tribe or people who gave thei...</td>\n",
       "      <td>0.647973</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1498</th>\n",
       "      <td>None</td>\n",
       "      <td>0.933831</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1499</th>\n",
       "      <td>None</td>\n",
       "      <td>0.890298</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1500 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           gen_response  response_time\n",
       "0     The HMS Plymouth is a frigate belonging to the...       0.792666\n",
       "1     The South Foreland Lighthouse is located in So...       0.813223\n",
       "2                                                  None       0.892982\n",
       "3                                                  None       0.898334\n",
       "4                                                  None       0.895691\n",
       "...                                                 ...            ...\n",
       "1495                                               None       0.881157\n",
       "1496                                               None       0.904275\n",
       "1497  The Vulci were a tribe or people who gave thei...       0.647973\n",
       "1498                                               None       0.933831\n",
       "1499                                               None       0.890298\n",
       "\n",
       "[1500 rows x 2 columns]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "import json\n",
    "import pandas as pd\n",
    "\n",
    "# Define functions\n",
    "def find_first_valid_json(text):\n",
    "    if not isinstance(text, str) or text.strip() == \"\":\n",
    "        return None  # skip if not a string or empty string\n",
    "    \n",
    "    json_objects = re.findall(r'\\{.*?\\}', text, re.DOTALL)\n",
    "    for obj in json_objects:\n",
    "        try:\n",
    "            json_obj = json.loads(obj)\n",
    "            if \"response\" in json_obj:  # Only check for \"response\"\n",
    "                return json_obj\n",
    "        except json.JSONDecodeError:\n",
    "            continue\n",
    "    return None\n",
    "\n",
    "def get_response(text):\n",
    "    if text is not None:  # Check if text is not None\n",
    "        try:\n",
    "            return text['response']\n",
    "        except (ValueError, SyntaxError, KeyError):\n",
    "            return None\n",
    "    return None\n",
    "\n",
    "\n",
    "# Replace empty strings in 'gen_response' with None\n",
    "ds.loc[ds['gen_response'] == '', 'gen_response'] = None\n",
    "\n",
    "# Apply the find_first_valid_json function\n",
    "ds['gen_response'] = ds['gen_response'].apply(lambda x: find_first_valid_json(x))\n",
    "\n",
    "# Convert gen_response to None if it's not a valid string\n",
    "ds['gen_response'] = ds['gen_response'].apply(lambda x: None if pd.isna(x) or x == 'nan' or isinstance(x, float) else x)\n",
    "\n",
    "# Extract 'response' from the JSON objects\n",
    "ds['gen_response'] = ds['gen_response'].apply(lambda x: get_response(x))\n",
    "\n",
    "# Keep the 'response_time' column unchanged\n",
    "ds['response_time'] = ds['response_time']\n",
    "\n",
    "# Define the new column order\n",
    "new_column_order = ['gen_response', 'response_time']\n",
    "\n",
    "# Reorder the columns\n",
    "ds = ds[new_column_order]\n",
    "\n",
    "print(\"\\nMissing Values:\\n\", ds.isnull().sum())\n",
    "print(ds.shape)\n",
    "ds\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds.to_csv(f'Responses/{DATASET}/{LLM}-train{COT_}.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
