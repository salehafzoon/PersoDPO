{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LLM Loading"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Auth Token Setting:\n",
    "\n",
    "- HugginigFace Token\n",
    "- OpenAI Token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.11/site-packages/scipy/__init__.py:155: UserWarning: A NumPy version >=1.18.5 and <1.26.0 is required for this version of SciPy (detected version 1.26.4\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n",
      "/opt/anaconda3/lib/python3.11/site-packages/pandas/core/arrays/masked.py:60: UserWarning: Pandas requires version '1.3.6' or newer of 'bottleneck' (version '1.3.5' currently installed).\n",
      "  from pandas.core import (\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline\n",
    "from dotenv import load_dotenv\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "import warnings\n",
    "import logging\n",
    "import torch\n",
    "import os\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "logging.getLogger('transformers').setLevel(logging.ERROR)\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "hf_token = os.getenv(\"HF_TOKEN\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "llms_info = {\n",
    "    \"Mistral-7B-Instruct\": {\n",
    "        \"remote_model_name\": \"mistralai/Mistral-7B-Instruct-v0.2\",\n",
    "        \"model_path\": \"./LLMs/Mistral-7B-Instruct\",\n",
    "        \"tokenizer_path\": \"./Tokenizers/Mistral-7B-Instruct\",\n",
    "        \"additional_config\": {\n",
    "            \"torch_dtype\": \"auto\",\n",
    "            \"device\": \"auto\"\n",
    "        }\n",
    "    },\n",
    "    \"Qwen2-7B-Instruct\": {\n",
    "        \"remote_model_name\": \"Qwen/Qwen2-7B-Instruct\",\n",
    "        \"model_path\": \"./LLMs/Qwen2-7B-Instruct\",\n",
    "        \"tokenizer_path\": \"./Tokenizers/Qwen2-7B-Instruct\",\n",
    "        \"additional_config\": {\n",
    "            \"torch_dtype\": \"auto\",\n",
    "            \"trust_remote_code\": True,\n",
    "            \"device\": \"auto\"\n",
    "        }\n",
    "    },\n",
    "    \"Qwen2-5B-Instruct\": {\n",
    "        \"remote_model_name\": \"Qwen/Qwen2-0.5B-Instruct\",\n",
    "        \"model_path\": \"./LLMs/Qwen2-5B-Instruct\",\n",
    "        \"tokenizer_path\": \"./Tokenizers/Qwen2-5B-Instruct\",\n",
    "        \"additional_config\":{\n",
    "            \"trust_remote_code\": True,\n",
    "        }\n",
    "    }, \n",
    "    \"Llama3-1-8B-Instruct\": {\n",
    "    \"remote_model_name\": \"meta-llama/Llama-3.1-8B-Instruct\",\n",
    "    \"model_path\": \"./LLMs/Llama3-1-8B-Instruct\",\n",
    "    \"tokenizer_path\": \"./Tokenizers/Llama3-1-8B-Instruct\",\n",
    "    # \"hf_token\": hf_token,\n",
    "    \"additional_config\": {\n",
    "        \"torch_dtype\": \"auto\",\n",
    "        \"device_map\": \"auto\",\n",
    "        \"rope_scaling\": {\n",
    "            \"type\": \"linear\",  # or \"dynamic\" — depending on your use case\n",
    "            \"factor\": 8.0\n",
    "        }\n",
    "    }\n",
    "},\n",
    "\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model(model_key):\n",
    "    model_info = llms_info[model_key]\n",
    "    config = model_info[\"additional_config\"]\n",
    "\n",
    "    # Check if the directories for the model and tokenizer exist\n",
    "    model_dir_exists = os.path.isdir(model_info[\"model_path\"])\n",
    "    tokenizer_dir_exists = os.path.isdir(model_info[\"tokenizer_path\"])\n",
    "\n",
    "    if model_dir_exists and tokenizer_dir_exists:\n",
    "        print(f\"{model_key} model and tokenizer are already present.\")\n",
    "    else:\n",
    "        print(f\"Downloading and saving model and tokenizer for {model_key}.\")\n",
    "        # Include the token in the download process if applicable\n",
    "        hf_token = model_info.get(\"hf_token\", None)\n",
    "        model = AutoModelForCausalLM.from_pretrained(\n",
    "            model_info[\"remote_model_name\"],\n",
    "            cache_dir=model_info[\"model_path\"],\n",
    "            torch_dtype=getattr(torch, config.get(\"torch_dtype\", \"auto\")) if config.get(\"torch_dtype\", \"auto\") != \"auto\" else None,\n",
    "            use_auth_token=hf_token\n",
    "        )\n",
    "        tokenizer = AutoTokenizer.from_pretrained(\n",
    "            model_info[\"remote_model_name\"],\n",
    "            cache_dir=model_info[\"tokenizer_path\"],\n",
    "            use_auth_token=hf_token\n",
    "        )\n",
    "        # Ensure directories are created during download\n",
    "        if not model_dir_exists:\n",
    "            os.makedirs(model_info[\"model_path\"], exist_ok=True)\n",
    "        if not tokenizer_dir_exists:\n",
    "            os.makedirs(model_info[\"tokenizer_path\"], exist_ok=True)\n",
    "        # Save them locally\n",
    "        model.save_pretrained(model_info[\"model_path\"])\n",
    "        tokenizer.save_pretrained(model_info[\"tokenizer_path\"])\n",
    "\n",
    "    # Load model and tokenizer from local storage\n",
    "    model = AutoModelForCausalLM.from_pretrained(model_info[\"model_path\"])\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_info[\"tokenizer_path\"])\n",
    "    return model, tokenizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load_model(\"Llama3-1-8B-Instruct\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Benchmarking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET = \"FoCus\"\n",
    "SET = \"train\"           #train,   valid       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "LLM = \"Mistral-7B-Instruct\"                  # Mistral-7B-Instruct, Llama3-1-8B-Instruct, Qwen2-7B-Instruct, Qwen2-5B-Instruct\n",
    "\n",
    "COT_SETUP = False                               "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Response Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1500, 3)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>personas</th>\n",
       "      <th>context</th>\n",
       "      <th>act_response</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>I want to learn more about the Royal Navy.I ha...</td>\n",
       "      <td>User1: I know this place, but I dont remember ...</td>\n",
       "      <td>User2: The shit was controversial he scrapped ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>I collect lighthouses.Ive never been to to Eng...</td>\n",
       "      <td>User1: Wow, this is amazing! What is this?\\nUs...</td>\n",
       "      <td>User2: Yes! An episode from ChuckleVision and ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I am interested in region.I would like to visi...</td>\n",
       "      <td>User1: Where is this place?\\nUser2: This is Ac...</td>\n",
       "      <td>User2: Waterways are essential to the commerci...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            personas  \\\n",
       "0  I want to learn more about the Royal Navy.I ha...   \n",
       "1  I collect lighthouses.Ive never been to to Eng...   \n",
       "2  I am interested in region.I would like to visi...   \n",
       "\n",
       "                                             context  \\\n",
       "0  User1: I know this place, but I dont remember ...   \n",
       "1  User1: Wow, this is amazing! What is this?\\nUs...   \n",
       "2  User1: Where is this place?\\nUser2: This is Ac...   \n",
       "\n",
       "                                        act_response  \n",
       "0  User2: The shit was controversial he scrapped ...  \n",
       "1  User2: Yes! An episode from ChuckleVision and ...  \n",
       "2  User2: Waterways are essential to the commerci...  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.empty_cache()\n",
    "\n",
    "# Loading the prompt\n",
    "df = pd.read_csv(f'./Prompts/{DATASET}-{SET}.csv')\n",
    "\n",
    "\n",
    "print(df.shape)\n",
    "df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to generate a prompt\n",
    "def create_benchmarking_prompt(personas, context, include_cot=False):\n",
    "\n",
    "    prompt = (\n",
    "        \"I will provide you with a conversation context and the personas of the participants, that can be annotated with speaker information.\\n\"\n",
    "        \"As a participant in this conversation, your task is to generate a personalized response, considering the conversation context and personas.\\n\\n\"\n",
    "        \"Participant Personas:\\n\"\n",
    "        f\"{personas}\\n\\n\"\n",
    "        \"Conversation Context:\\n\"\n",
    "        f\"{context}\\n\\n\"\n",
    "        \"Task Instruction:\\n\"\n",
    "        \"* Provide an unannotated response.\\n\"\n",
    "        \"* If only one persona is available, personalize the response accordingly.\\n\"\n",
    "        \"* If the conversation context is a single query, respond appropriately to the query.\\n\"\n",
    "    )\n",
    "    \n",
    "    if include_cot:\n",
    "        prompt += (\n",
    "            \"* Apply Chain of Thought reasoning to reflect on the alignment of your response with the personas.\\n\"\n",
    "        )\n",
    "\n",
    "    prompt += (\n",
    "        \"\\nOutput Format: only give a JSON of the following format:\\n\"\n",
    "        \"{\\n\"\n",
    "    )\n",
    "    \n",
    "    if include_cot:\n",
    "        prompt += (\n",
    "            '  \"reasoning\": \"briefly describe your personalization process (in 110 words or less).\"\\n'\n",
    "        )\n",
    "        \n",
    "    prompt += (\n",
    "        '  \"response\": \"provide the personalized natural language response here (in 110 words or less).\"\\n'\n",
    "        \"}\\n\"\n",
    "    )\n",
    "\n",
    "    return prompt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading and saving model and tokenizer for Mistral-7B-Instruct.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "56c6159f9b2547cf85bbe58cc33afdc2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/596 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "de23dd0fd5b145db86602b9b9b5d27e8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors.index.json:   0%|          | 0.00/25.1k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "40dcef1c1acc40a98da5bb2f5058ba56",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "14b9df88c7ce468ba4e24e86e9e4c3e6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00001-of-00003.safetensors:   0%|          | 0.00/4.94G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7e3a6a4b4cac473d810936745bd24c43",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00002-of-00003.safetensors:   0%|          | 0.00/5.00G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "64317f8e7a4d4ea49c0280c6fc938890",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00003-of-00003.safetensors:   0%|          | 0.00/4.54G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a1f2942efc2b42f795b0ceba820322cc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "563ee0c544114b2780bbfb3bd722e329",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/111 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "be0b607d54b948f7aa468fc605430b4e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Assuming load_model is defined and works as expected\n",
    "model, tokenizer = load_model(LLM)\n",
    "\n",
    "MAX_NEW_TOKEN = 220 if COT_SETUP else 110\n",
    "\n",
    "generation_params = {\n",
    "    \n",
    "    \"max_new_tokens\": MAX_NEW_TOKEN,      # Based on max response length + reasoning\n",
    "    \"temperature\": 0,                     # Based on FELM paper (Greedy Setup)\n",
    "    \"do_sample\": False\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I will provide you with a conversation context and the personas of the participants, that can be annotated with speaker information.\n",
      "As a participant in this conversation, your task is to generate a personalized response, considering the conversation context and personas.\n",
      "\n",
      "Participant Personas:\n",
      "I have the fantasy about mountain.I like hill fort.I have heard about Celtic centres.I would like to go to Germany.I like rivers.\n",
      "\n",
      "Conversation Context:\n",
      "User1: Wow, this is amazing! What is this?\n",
      "User2: This will be your favorite place. The Heuneburg is a prehistoric hillfort in Germany, the country you want to visit.\n",
      "User1: Nice! Where is exactly this place located?\n",
      "User2: Its located in Hundersingen near Herbertingen, between Ulm and Sigmaringen, Baden-Württemberg, in the south of Germany.\n",
      "User1: Tell me more about the surroundings of the site.\n",
      "User2: Its on the side of the river Danube. Also, it is near to the modern borders with Switzerland and Austria.\n",
      "User1: What is the significance of the site?\n",
      "User2: You have heard about Celtic centres; the fort is considered to be one of the most important early Celtic centres in Central Europe.\n",
      "User1: When the fort was abandoned?\n",
      "User2: The fort was abandoned in the 5th century BC.\n",
      "User1: What is the height of the fort above the river?\n",
      "\n",
      "Task Instruction:\n",
      "* Provide an unannotated response.\n",
      "* If only one persona is available, personalize the response accordingly.\n",
      "* If the conversation context is a single query, respond appropriately to the query.\n",
      "\n",
      "Output Format: only give a JSON of the following format:\n",
      "{\n",
      "  \"response\": \"provide the personalized natural language response here (in 110 words or less).\"\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "first_row = df.iloc[120]\n",
    "personas = first_row['personas']\n",
    "context = first_row['context']\n",
    "\n",
    "# Example usage\n",
    "prompt = create_benchmarking_prompt(personas, context, include_cot=COT_SETUP)\n",
    "print(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Unannotated Response:\n",
      "{\n",
      "  \"response\": \"The Heuneburg hillfort is located in Hundersingen, Germany, near the Danube river. It's considered one of the most important early Celtic centres in Central Europe, abandoned in the 5th century BC. The site offers a stunning view of the surrounding landscape and is near the modern borders with Switzerland and Austria.\"\n",
      "}\n",
      "\n",
      "Annotated Response (for User1):\n",
      "{\n",
      "  \"response\": \"The Heuneb\n"
     ]
    }
   ],
   "source": [
    "# Initialize the text generation pipeline\n",
    "generator = pipeline('text-generation', model=model, tokenizer=tokenizer, device=device)\n",
    "\n",
    "# Generate a response\n",
    "output = generator(prompt, **generation_params)\n",
    "response = output[0]['generated_text'][len(prompt):]\n",
    "\n",
    "# Print the response\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating responses: 100%|██████████| 1500/1500 [1:51:04<00:00,  4.44s/it]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>gen_response</th>\n",
       "      <th>response_time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>\\nResponse:\\n{\\n  \"response\": \"The HMS Plymout...</td>\n",
       "      <td>4.395349</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>\\nUnannotated Response:\\n{\\n  \"response\": \"The...</td>\n",
       "      <td>4.356761</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>\\nUnannotated Response:\\n{\\n  \"response\": \"Aca...</td>\n",
       "      <td>4.653390</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>\\nResponse:\\n{\\n  \"response\": \"Vail Ski Resort...</td>\n",
       "      <td>4.440063</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>\\n{\\n  \"response\": \"Mesa Verde National Park, ...</td>\n",
       "      <td>4.460657</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>\\nUnannotated Response:\\n{\\n  \"response\": \"Gre...</td>\n",
       "      <td>4.370527</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>\\nUnannotated Response:\\n{\\n  \"response\": \"The...</td>\n",
       "      <td>4.379969</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>\\nOutput:\\n{\\n  \"response\": \"Harlaxton Manor i...</td>\n",
       "      <td>4.463670</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>\\nUnannotated Response:\\n{\\n  \"response\": \"The...</td>\n",
       "      <td>4.526228</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>\\nResponse:\\n{\\n  \"response\": \"Wat Traphang Th...</td>\n",
       "      <td>4.131498</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>\\n{\\n  \"response\": \"The Sigüenza Cathedral in ...</td>\n",
       "      <td>4.419158</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>\\nOutput:\\n{\\n  \"response\": \"The College Hill ...</td>\n",
       "      <td>4.278567</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>\\nOutput:\\n{\\n  \"response\": \"The Rudd Residenc...</td>\n",
       "      <td>4.310547</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>\\nUnannotated Response:\\n{\\n  \"response\": \"Mou...</td>\n",
       "      <td>4.336081</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>\\nUnannotated Response:\\n{\\n  \"response\": \"The...</td>\n",
       "      <td>4.302974</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>\\nUnannotated Response:\\n{\\n  \"response\": \"A h...</td>\n",
       "      <td>4.380060</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>\\nUnannotated Response:\\n{\\n  \"response\": \"The...</td>\n",
       "      <td>4.269793</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>\\n{\\n  \"response\": \"Although you might not enj...</td>\n",
       "      <td>4.447322</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>\\nUnannotated Response:\\n{\\n  \"response\": \"The...</td>\n",
       "      <td>4.414347</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>\\nUnannotated Response:\\n{\\n  \"response\": \"Woo...</td>\n",
       "      <td>4.428581</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         gen_response  response_time\n",
       "0   \\nResponse:\\n{\\n  \"response\": \"The HMS Plymout...       4.395349\n",
       "1   \\nUnannotated Response:\\n{\\n  \"response\": \"The...       4.356761\n",
       "2   \\nUnannotated Response:\\n{\\n  \"response\": \"Aca...       4.653390\n",
       "3   \\nResponse:\\n{\\n  \"response\": \"Vail Ski Resort...       4.440063\n",
       "4   \\n{\\n  \"response\": \"Mesa Verde National Park, ...       4.460657\n",
       "5   \\nUnannotated Response:\\n{\\n  \"response\": \"Gre...       4.370527\n",
       "6   \\nUnannotated Response:\\n{\\n  \"response\": \"The...       4.379969\n",
       "7   \\nOutput:\\n{\\n  \"response\": \"Harlaxton Manor i...       4.463670\n",
       "8   \\nUnannotated Response:\\n{\\n  \"response\": \"The...       4.526228\n",
       "9   \\nResponse:\\n{\\n  \"response\": \"Wat Traphang Th...       4.131498\n",
       "10  \\n{\\n  \"response\": \"The Sigüenza Cathedral in ...       4.419158\n",
       "11  \\nOutput:\\n{\\n  \"response\": \"The College Hill ...       4.278567\n",
       "12  \\nOutput:\\n{\\n  \"response\": \"The Rudd Residenc...       4.310547\n",
       "13  \\nUnannotated Response:\\n{\\n  \"response\": \"Mou...       4.336081\n",
       "14  \\nUnannotated Response:\\n{\\n  \"response\": \"The...       4.302974\n",
       "15  \\nUnannotated Response:\\n{\\n  \"response\": \"A h...       4.380060\n",
       "16  \\nUnannotated Response:\\n{\\n  \"response\": \"The...       4.269793\n",
       "17  \\n{\\n  \"response\": \"Although you might not enj...       4.447322\n",
       "18  \\nUnannotated Response:\\n{\\n  \"response\": \"The...       4.414347\n",
       "19  \\nUnannotated Response:\\n{\\n  \"response\": \"Woo...       4.428581"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "# Iterate through the DataFrame and generate responses\n",
    "gen_responses = []\n",
    "response_times = []\n",
    "\n",
    "for index, row in tqdm(df.iterrows(), total=len(df), desc=\"Generating responses\"):\n",
    "    personas = row['personas']\n",
    "    context = row['context']\n",
    "    \n",
    "    # Create the prompt\n",
    "    prompt = create_benchmarking_prompt(personas, context, COT_SETUP)\n",
    "    \n",
    "    # Measure the start time\n",
    "    start_time = time.time()\n",
    "    \n",
    "    # Generate a response\n",
    "    output = generator(prompt, **generation_params)[0]['generated_text']\n",
    "    \n",
    "    # Measure the end time and calculate the duration\n",
    "    end_time = time.time()\n",
    "    response_time = end_time - start_time\n",
    "    \n",
    "    response = output[len(prompt):]\n",
    "\n",
    "    # Store the generated response and response time\n",
    "    gen_responses.append(response)\n",
    "    response_times.append(response_time)\n",
    "\n",
    "# Create a DataFrame with the responses and response times\n",
    "response_df = pd.DataFrame({\n",
    "    'gen_response': gen_responses,\n",
    "    'response_time': response_times\n",
    "})  \n",
    "\n",
    "COT_ = \"-COT\" if COT_SETUP else \"\"\n",
    "\n",
    "response_df.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Missing Values:\n",
      " gen_response     0\n",
      "response_time    0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nMissing Values:\\n\", response_df.isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Save the response DataFrame to a CSV and Excel file\n",
    "response_df.to_csv(f'./Raw Responses/{DATASET}/{LLM}-train{COT_}.csv', index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Post Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape: (1500, 2)\n",
      "\n",
      "Missing Values:\n",
      " gen_response     0\n",
      "response_time    0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "ds = response_df\n",
    "print(\"Shape:\", ds.shape)\n",
    "\n",
    "print(\"\\nMissing Values:\\n\", ds.isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Missing Values:\n",
      " gen_response     928\n",
      "response_time      0\n",
      "dtype: int64\n",
      "(1500, 2)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>gen_response</th>\n",
       "      <th>response_time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>None</td>\n",
       "      <td>4.395349</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>None</td>\n",
       "      <td>4.356761</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>None</td>\n",
       "      <td>4.653390</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Vail Ski Resort in Colorado is the perfect pla...</td>\n",
       "      <td>4.440063</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>None</td>\n",
       "      <td>4.460657</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1495</th>\n",
       "      <td>None</td>\n",
       "      <td>4.504809</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1496</th>\n",
       "      <td>None</td>\n",
       "      <td>4.504092</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1497</th>\n",
       "      <td>None</td>\n",
       "      <td>4.736598</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1498</th>\n",
       "      <td>None</td>\n",
       "      <td>5.180323</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1499</th>\n",
       "      <td>None</td>\n",
       "      <td>4.506885</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1500 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           gen_response  response_time\n",
       "0                                                  None       4.395349\n",
       "1                                                  None       4.356761\n",
       "2                                                  None       4.653390\n",
       "3     Vail Ski Resort in Colorado is the perfect pla...       4.440063\n",
       "4                                                  None       4.460657\n",
       "...                                                 ...            ...\n",
       "1495                                               None       4.504809\n",
       "1496                                               None       4.504092\n",
       "1497                                               None       4.736598\n",
       "1498                                               None       5.180323\n",
       "1499                                               None       4.506885\n",
       "\n",
       "[1500 rows x 2 columns]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "import json\n",
    "import pandas as pd\n",
    "\n",
    "# Define functions\n",
    "def find_first_valid_json(text):\n",
    "    if not isinstance(text, str) or text.strip() == \"\":\n",
    "        return None  # skip if not a string or empty string\n",
    "    \n",
    "    json_objects = re.findall(r'\\{.*?\\}', text, re.DOTALL)\n",
    "    for obj in json_objects:\n",
    "        try:\n",
    "            json_obj = json.loads(obj)\n",
    "            if \"response\" in json_obj:  # Only check for \"response\"\n",
    "                return json_obj\n",
    "        except json.JSONDecodeError:\n",
    "            continue\n",
    "    return None\n",
    "\n",
    "def get_response(text):\n",
    "    if text is not None:  # Check if text is not None\n",
    "        try:\n",
    "            return text['response']\n",
    "        except (ValueError, SyntaxError, KeyError):\n",
    "            return None\n",
    "    return None\n",
    "\n",
    "\n",
    "# Replace empty strings in 'gen_response' with None\n",
    "ds.loc[ds['gen_response'] == '', 'gen_response'] = None\n",
    "\n",
    "# Apply the find_first_valid_json function\n",
    "ds['gen_response'] = ds['gen_response'].apply(lambda x: find_first_valid_json(x))\n",
    "\n",
    "# Convert gen_response to None if it's not a valid string\n",
    "ds['gen_response'] = ds['gen_response'].apply(lambda x: None if pd.isna(x) or x == 'nan' or isinstance(x, float) else x)\n",
    "\n",
    "# Extract 'response' from the JSON objects\n",
    "ds['gen_response'] = ds['gen_response'].apply(lambda x: get_response(x))\n",
    "\n",
    "# Keep the 'response_time' column unchanged\n",
    "ds['response_time'] = ds['response_time']\n",
    "\n",
    "# Define the new column order\n",
    "new_column_order = ['gen_response', 'response_time']\n",
    "\n",
    "# Reorder the columns\n",
    "ds = ds[new_column_order]\n",
    "\n",
    "print(\"\\nMissing Values:\\n\", ds.isnull().sum())\n",
    "print(ds.shape)\n",
    "ds\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds.to_csv(f'Responses/{DATASET}/{LLM}-train{COT_}.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
